{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a11d66fc-f9ef-4449-953b-c1dcba973a7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "project video: https://drive.google.com/file/d/1_a1fiIlA9ay8liPMs7YG3IaWuz35mbCo/view?usp=sharing"
      ],
      "metadata": {
        "id": "OS5PbIiAZDFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This report details the reproduction of the paper titled \"Time-Aware Transformer-based Network for Clinical Notes Series Prediction\" by Dongyu Zhang et al [1]. The original paper proposes a novel architecture, FTL-Trans, designed to leverage both time-aware and hierarchical model structures for predicting clinical outcomes from patient notes.\n",
        "\n",
        "*   Background of the problem\n",
        "  * Clinical note analysis is crucial for automated decision-making systems in healthcare. These notes, which chronicle patient status, are typically written in unstructured text over the course of a hospital stay. The paper addresses the challenge of extracting meaningful predictions from these temporally and contextually rich documents.\n",
        "*   Paper explanation\n",
        "  * The original work introduces a hierarchical transformer-based model that incorporates time-aware LSTM cells (FT-LSTM) to handle the sequence and temporal nature of clinical notes. It claims to significantly outperform previous state-of-the-art models on tasks like mortality and readmission prediction, showing improvements in both AUROC and accuracy.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "\n",
        "1. Hypothesis 1: Models incorporating both embedding modes (no embedding vs all embedding) show variance in performance.\n",
        "2. Hypothesis 2: Time-aware architectures like TL-Trans improve prediction metrics compared to traditional transformer-based models.\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "\n",
        "### Language\n",
        "Python3 == 3.x.x\n",
        "### Module\n",
        "torch==1.3.1+cu92\n",
        "\n",
        "pytorch-pretrained-bert==0.6.2\n",
        "\n",
        "pytorch-transformers==1.2.0\n",
        "\n",
        "tqdm==4.37.0\n",
        "\n",
        "dotmap==1.3.8\n",
        "\n",
        "six==1.13.0\n",
        "\n",
        "matplotlib==3.1.1\n",
        "\n",
        "numpy==1.17.3\n",
        "\n",
        "pandas==0.25.3"
      ],
      "metadata": {
        "id": "J9muYVAHUDAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "  * Source of the data: The data for this project comes from the [MIMIC-III](https://mimic.mit.edu/docs/gettingstarted/) database [3], which includes de-identified health-related data associated with over forty thousand patients who stayed in intensive care units. Download instruction can be found in the link.\n",
        "  * Statistics: Part of the dataset is randomly selected and splitted into training, validation, and testing sets. Here are the specific details:\n",
        "    * Training Set: 4929\n",
        "    * Validation Set: 479\n",
        "    * Testing Set: 768\n",
        "  \n",
        "  The label distribution across the dataset (1 for E. coli infection and 0 for not E. coli infected) was balanced as part of the preprocessing to ensure the model training process was not biased towards a particular outcome.\n",
        "  * Data process: The data preprocessing involved several steps implemented in the preprocessecoli.py script:\n",
        "\n",
        "    * Data Cleaning: The clinical notes were cleaned to remove any non-relevant text, punctuation, and transformed to lowercase to standardize the input for model training.\n",
        "    * Tokenization: I used the BERT tokenizer to convert text into tokens that could be fed into the model. This process converts each word into a unique integer.\n",
        "    * Chunking: Each clinical note was divided into segments or \"chunks\" of text to accommodate the model's input size limitations.\n",
        "    * Splitting Data: The dataset was divided into training, validation, and test sets to evaluate the model's performance on unseen data.\n",
        "  * Illustration: Following is the illustration of train dataset.\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import re\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# def preprocess1(x):\n",
        "#     y = re.sub('\\\\[(.*?)\\\\]', '', x)  # remove de-identified brackets\n",
        "#     y = re.sub('[0-9]+\\.', '', y)  # remove 1.2. since the segmenter segments based on this\n",
        "#     y = re.sub('dr\\.', 'doctor', y)\n",
        "#     y = re.sub('m\\.d\\.', 'md', y)\n",
        "#     y = re.sub('admission date:', '', y)\n",
        "#     y = re.sub('discharge date:', '', y)\n",
        "#     y = re.sub('--|__|==', '', y)\n",
        "#     return y\n",
        "\n",
        "\n",
        "# def preprocessing(df_less_n, tokenizer):\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].fillna(' ')\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\n', ' ')\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\r', ' ')\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].apply(str.strip)\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].str.lower()\n",
        "\n",
        "#     df_less_n['TEXT'] = df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
        "\n",
        "#     sen = df_less_n['TEXT'].values\n",
        "#     tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "#     # print(\"First sentence tokenized\")\n",
        "#     # print(tokenized_texts[0])\n",
        "#     input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "#     df_less_n['Input_ID'] = input_ids\n",
        "#     return df_less_n[['Adm_ID', 'Note_ID', 'TEXT', 'Input_ID', 'Label', 'chartdate', 'charttime']]\n",
        "\n",
        "\n",
        "# def main():\n",
        "    # df = pd.read_csv(\"MICROBIOLOGYEVENTS.csv\").fillna(\"\")\n",
        "    # df_ecoli = df.loc[df['ORG_ITEMID'] == 80002]\n",
        "    # df_ecoli.to_csv(\"ecoli.csv\")\n",
        "    # df_nonecoli = df.loc[df['ORG_ITEMID'] != 80002]\n",
        "    # df_nonecoli.to_csv(\"nonecoli.csv\")\n",
        "    #\n",
        "    # df_ecoli = pd.read_csv(\"ecoli.csv\")\n",
        "    # ecoli_id = df_ecoli['HADM_ID'].unique()\n",
        "    # df_nonecoli = pd.read_csv(\"nonecoli.csv\")\n",
        "    # nonecoli_id = df_nonecoli['HADM_ID'].unique()\n",
        "    #\n",
        "    original_df = pd.read_csv('NOTEEVENTS.csv')\n",
        "    # keep_col = [\"ROW_ID\",\"HADM_ID\",\"CHARTDATE\",\"CHARTTIME\",\"TEXT\"]\n",
        "    # new_f = original_df[keep_col]\n",
        "    # for i in range(int(np.ceil(len(new_f) / 10000))):\n",
        "    #     label = []\n",
        "    #     df_chunk = new_f.iloc[i * 10000:(i + 1) * 10000].copy()\n",
        "    #     for index, row in df_chunk.iterrows():\n",
        "    #         if row['HADM_ID'] in ecoli_id:\n",
        "    #             label.append(1)\n",
        "    #         elif row['HADM_ID'] in nonecoli_id:\n",
        "    #             label.append(0)\n",
        "    #         else:\n",
        "    #             df_chunk.drop(index, inplace=True)\n",
        "    #     df_chunk['LABEL'] = label\n",
        "    #     temp_file_dir = './tmp/Processed_{}.csv'.format(i)\n",
        "    #     df_chunk.to_csv(temp_file_dir, index=False)\n",
        "\n",
        "    # tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "    # for i in range(int(np.ceil(len(original_df) / 10000))):\n",
        "    #     temp_file_dir = './tmp/Processed_{}.csv'.format(i)\n",
        "    #     df_chunk = pd.read_csv(temp_file_dir, header=0)\n",
        "    #     df_processed_chunk = df_chunk[['HADM_ID', 'ROW_ID', 'CHARTDATE', 'CHARTTIME', 'TEXT','LABEL']]\n",
        "    #     df_processed_chunk.rename(columns={'HADM_ID': \"Adm_ID\",\n",
        "    #                                 'ROW_ID': \"Note_ID\",\n",
        "    #                                 'CHARTDATE': \"chartdate\",\n",
        "    #                                 'CHARTTIME': \"charttime\",\n",
        "    #                                 'TEXT': \"TEXT\",\n",
        "    #                                 'LABEL': \"Label\"}, inplace=True)\n",
        "    #     df_processed_chunk = df_processed_chunk.astype({'Adm_ID': 'int64', 'Note_ID': 'int64', 'Label': 'int64'})\n",
        "    #     df_processed_chunk = preprocessing(df_processed_chunk, tokenizer)\n",
        "    #     temp_file_dir = './tmp1/Processed_{}.csv'.format(i)\n",
        "    #     df_processed_chunk.to_csv(temp_file_dir, index=False)\n",
        "\n",
        "    # df = pd.DataFrame({'Adm_ID': [], 'Note_ID': [], 'TEXT': [], 'Input_ID': [],\n",
        "    #                    'Label': [], 'chartdate': [], 'charttime': []})\n",
        "    # for i in range(40):\n",
        "    #     temp_file_dir = './tmp1/Processed_{}.csv'.format(i)\n",
        "    #     df_chunk = pd.read_csv(temp_file_dir, header=0)\n",
        "    #     df = df.append(df_chunk, ignore_index=True)\n",
        "    # df.to_csv('./Ecoli/data.csv', index=False)\n",
        "    #\n",
        "    # result = df.Label.value_counts()\n",
        "    # print(\"In the full dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\".format(result[1], result[0]))\n",
        "    # ecoli_ID = pd.Series(df[df.Label == 1].Adm_ID.unique())\n",
        "    # not_ecoli_ID = pd.Series(df[df.Label == 0].Adm_ID.unique())\n",
        "    # print(\"Total Positive Patients' ids: {}, Total Negative Patients' ids: {}\".format(len(ecoli_ID), len(not_ecoli_ID)))\n",
        "\n",
        "    # df = pd.read_csv('./Ecoli/data.csv', header=0)\n",
        "    # ecoli_ID = pd.Series(df[df.Label == 1].Adm_ID.unique())\n",
        "    # not_ecoli_ID = pd.Series(df[df.Label == 0].Adm_ID.unique())\n",
        "    #\n",
        "    # RANDOM_SEED = 1\n",
        "    # not_ecoli_ID_use = not_ecoli_ID.sample(n=500, random_state=RANDOM_SEED)\n",
        "    # ecoli_ID_use = ecoli_ID.sample(n=500, random_state=RANDOM_SEED)\n",
        "    #\n",
        "    # Kfold = None\n",
        "    # if Kfold is None:\n",
        "    #     id_val_test_t = ecoli_ID_use.sample(frac=0.2, random_state=RANDOM_SEED)\n",
        "    #     id_val_test_f = not_ecoli_ID_use.sample(frac=0.2, random_state=RANDOM_SEED)\n",
        "    #\n",
        "    #     id_train_t = ecoli_ID_use.drop(id_val_test_t.index)\n",
        "    #     id_train_f = not_ecoli_ID_use.drop(id_val_test_f.index)\n",
        "    #\n",
        "    #     id_val_t = id_val_test_t.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "    #     id_test_t = id_val_test_t.drop(id_val_t.index)\n",
        "    #     id_val_f = id_val_test_f.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "    #     id_test_f = id_val_test_f.drop(id_val_f.index)\n",
        "    #\n",
        "    #     id_test = pd.concat([id_test_t, id_test_f])\n",
        "    #     test_id_label = pd.DataFrame(data=list(zip(id_test, [1] * len(id_test_t) + [0] * len(id_test_f))),\n",
        "    #                                  columns=['id', 'label'])\n",
        "    #\n",
        "    #     id_val = pd.concat([id_val_t, id_val_f])\n",
        "    #     val_id_label = pd.DataFrame(data=list(zip(id_val, [1] * len(id_val_t) + [0] * len(id_val_f))),\n",
        "    #                                 columns=['id', 'label'])\n",
        "    #\n",
        "    #     id_train = pd.concat([id_train_t, id_train_f])\n",
        "    #     train_id_label = pd.DataFrame(data=list(zip(id_train, [1] * len(id_train_t) + [0] * len(id_train_f))),\n",
        "    #                                   columns=['id', 'label'])\n",
        "    #\n",
        "    #     mortality_train = df[df.Adm_ID.isin(train_id_label.id)]\n",
        "    #     mortality_val = df[df.Adm_ID.isin(val_id_label.id)]\n",
        "    #     mortality_test = df[df.Adm_ID.isin(test_id_label.id)]\n",
        "    #     mortality_not_use = df[\n",
        "    #         (~df.Adm_ID.isin(train_id_label.id)) & (\n",
        "    #                     ~df.Adm_ID.isin(val_id_label.id) & (~df.Adm_ID.isin(test_id_label.id)))]\n",
        "    #\n",
        "    #     train_result = mortality_train.Label.value_counts()\n",
        "    #\n",
        "    #     val_result = mortality_val.Label.value_counts()\n",
        "    #\n",
        "    #     test_result = mortality_test.Label.value_counts()\n",
        "    #\n",
        "    #     no_result = mortality_not_use.Label.value_counts()\n",
        "    #\n",
        "    #     mortality_train.to_csv('./Ecoli/train.csv', index=False)\n",
        "    #     mortality_val.to_csv('./Ecoli/val.csv', index=False)\n",
        "    #     mortality_test.to_csv('./Ecoli/test.csv', index=False)\n",
        "    #     mortality_not_use.to_csv('./Ecoli/not_use.csv', index=False)\n",
        "    #     df.to_csv('./Ecoli/full.csv', index=False)\n",
        "    #\n",
        "    #     if len(no_result) == 2:\n",
        "    #         print((\"In the train dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                \"In the not use dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\").format(\n",
        "    #             train_result[1],\n",
        "    #             train_result[0],\n",
        "    #             val_result[1],\n",
        "    #             val_result[0],\n",
        "    #             test_result[1],\n",
        "    #             test_result[0],\n",
        "    #             no_result[1],\n",
        "    #             no_result[0]))\n",
        "    #     else:\n",
        "    #         try:\n",
        "    #             print((\"In the train dataset Positive Patients' Notes: {}, Negative  Patients' Notes: {}\\n\"\n",
        "    #                    \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the not use dataset Negative Patients' Notes: {}\").format(train_result[1],\n",
        "    #                                                                                  train_result[0],\n",
        "    #                                                                                  val_result[1],\n",
        "    #                                                                                  val_result[0],\n",
        "    #                                                                                  test_result[1],\n",
        "    #                                                                                  test_result[0],\n",
        "    #                                                                                  no_result[0])\n",
        "    #                   )\n",
        "    #         except KeyError:\n",
        "    #             print((\"In the train dataset Positive Patients' Notes: {}, Negative  Patients' Notes: {}\\n\"\n",
        "    #                    \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the not use dataset Positive Patients' Notes: {}\").format(train_result[1],\n",
        "    #                                                                                  train_result[0],\n",
        "    #                                                                                  val_result[1],\n",
        "    #                                                                                  val_result[0],\n",
        "    #                                                                                  test_result[1],\n",
        "    #                                                                                  test_result[0],\n",
        "    #                                                                                  no_result[1])\n",
        "    #                   )\n",
        "    #\n",
        "    # else:\n",
        "    #     folds_t = KFold(Kfold, False, RANDOM_SEED)\n",
        "    #     folds_f = KFold(Kfold, False, RANDOM_SEED)\n",
        "    #     ecoli_ID_use.reset_index(inplace=True, drop=True)\n",
        "    #     not_ecoli_ID_use.reset_index(inplace=True, drop=True)\n",
        "    #     for num, ((train_t, test_t), (train_f, test_f)) in enumerate(zip(folds_t.split(ecoli_ID_use),\n",
        "    #                                                                      folds_f.split(not_ecoli_ID_use))):\n",
        "    #         id_train_t = ecoli_ID_use[train_t]\n",
        "    #         id_val_test_t = ecoli_ID_use[test_t]\n",
        "    #         id_train_f = not_ecoli_ID_use[train_f]\n",
        "    #         id_val_test_f = not_ecoli_ID_use[test_f]\n",
        "    #         id_val_t = id_val_test_t.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "    #         id_test_t = id_val_test_t.drop(id_val_t.index)\n",
        "    #         id_val_f = id_val_test_f.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "    #         id_test_f = id_val_test_f.drop(id_val_f.index)\n",
        "    #\n",
        "    #         id_test = pd.concat([id_test_t, id_test_f])\n",
        "    #         test_id_label = pd.DataFrame(data=list(zip(id_test, [1] * len(id_test_t) + [0] * len(id_test_f))),\n",
        "    #                                      columns=['id', 'label'])\n",
        "    #\n",
        "    #         id_val = pd.concat([id_val_t, id_val_f])\n",
        "    #         val_id_label = pd.DataFrame(data=list(zip(id_val, [1] * len(id_val_t) + [0] * len(id_val_f))),\n",
        "    #                                     columns=['id', 'label'])\n",
        "    #\n",
        "    #         id_train = pd.concat([id_train_t, id_train_f])\n",
        "    #         train_id_label = pd.DataFrame(data=list(zip(id_train, [1] * len(id_train_t) + [0] * len(id_train_f))),\n",
        "    #                                       columns=['id', 'label'])\n",
        "    #\n",
        "    #         mortality_train = df[df.Adm_ID.isin(train_id_label.id)]\n",
        "    #         mortality_val = df[df.Adm_ID.isin(val_id_label.id)]\n",
        "    #         mortality_test = df[df.Adm_ID.isin(test_id_label.id)]\n",
        "    #         mortality_not_use = df[\n",
        "    #             (~df.Adm_ID.isin(train_id_label.id)) & (\n",
        "    #                     ~df.Adm_ID.isin(val_id_label.id) & (~df.Adm_ID.isin(test_id_label.id)))]\n",
        "    #\n",
        "    #         train_result = mortality_train.Label.value_counts()\n",
        "    #\n",
        "    #         val_result = mortality_val.Label.value_counts()\n",
        "    #\n",
        "    #         test_result = mortality_test.Label.value_counts()\n",
        "    #\n",
        "    #         no_result = mortality_not_use.Label.value_counts()\n",
        "    #\n",
        "    #         mortality_train.to_csv('./Ecoli/train.csv', index=False)\n",
        "    #         mortality_val.to_csv('./Ecoli/val.csv', index=False)\n",
        "    #         mortality_test.to_csv('./Ecoli/test.csv', index=False)\n",
        "    #         mortality_not_use.to_csv('./Ecoli/not_use.csv', index=False)\n",
        "    #         df.to_csv(os.path.join('./Ecoli/', str(num), 'full.csv'), index=False)\n",
        "    #\n",
        "    #         if len(no_result) == 2:\n",
        "    #             print((\"In the {}th split of {} folds\\n\"\n",
        "    #                    \"In the train dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                    \"In the not use dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\").format(\n",
        "    #                 num,\n",
        "    #                 Kfold,\n",
        "    #                 train_result[1],\n",
        "    #                 train_result[0],\n",
        "    #                 val_result[1],\n",
        "    #                 val_result[0],\n",
        "    #                 test_result[1],\n",
        "    #                 test_result[0],\n",
        "    #                 no_result[1],\n",
        "    #                 no_result[0])\n",
        "    #             )\n",
        "    #         else:\n",
        "    #             try:\n",
        "    #                 print((\"In the {}th split of {} folds\\n\"\n",
        "    #                        \"In the train dataset Positive Patients' Notes: {}, Negative  Patients' Notes: {}\\n\"\n",
        "    #                        \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                        \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                        \"In the not use dataset Negative Patients' Notes: {}\").format(num,\n",
        "    #                                                                                      Kfold,\n",
        "    #                                                                                      train_result[1],\n",
        "    #                                                                                      train_result[0],\n",
        "    #                                                                                      val_result[1],\n",
        "    #                                                                                      val_result[0],\n",
        "    #                                                                                      test_result[1],\n",
        "    #                                                                                      test_result[0],\n",
        "    #                                                                                      no_result[0])\n",
        "    #                       )\n",
        "    #             except KeyError:\n",
        "    #                 print((\"In the {}th split of {} folds\\n\"\n",
        "    #                        \"In the train dataset Positive Patients' Notes: {}, Negative  Patients' Notes: {}\\n\"\n",
        "    #                        \"In the validation dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                        \"In the test dataset Positive Patients' Notes: {}, Negative Patients' Notes: {}\\n\"\n",
        "    #                        \"In the not use dataset Positive Patients' Notes: {}\").format(num,\n",
        "    #                                                                                      Kfold,\n",
        "    #                                                                                      train_result[1],\n",
        "    #                                                                                      train_result[0],\n",
        "    #                                                                                      val_result[1],\n",
        "    #                                                                                      val_result[0],\n",
        "    #                                                                                      test_result[1],\n",
        "    #                                                                                      test_result[0],\n",
        "    #                                                                                      no_result[1])\n",
        "    #                       )\n",
        "\n",
        "    # df = pd.read_csv(\"./Ecoli/train1.csv\")\n",
        "    # df = df.drop('TEXT', axis=1)\n",
        "    # df.to_csv('./Ecoli/train.csv', index=False)\n",
        "    #\n",
        "    # df1 = pd.read_csv(\"./Ecoli/test1.csv\")\n",
        "    # df1 = df1.drop('TEXT', axis=1)\n",
        "    # df1.to_csv('./Ecoli/test.csv', index=False)\n",
        "\n",
        "#     df2 = pd.read_csv(\"./Ecoli/val1.csv\")\n",
        "#     df2 = df2.drop('TEXT', axis=1)\n",
        "#     df2.to_csv('./Ecoli/val.csv', index=False)\n",
        "#\n",
        "#\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "id": "nhN2Jl27U6Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BZScZNbROw-N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "outputId": "a53da9f2-716c-47e4-e462-9e46229eb2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Adm_ID  Note_ID                                           Input_ID  \\\n",
            "0  139574.0    185.0  [2326, 1024, 4200, 2035, 2121, 17252, 1024, 56...   \n",
            "1  162436.0    195.0  [2326, 1024, 4200, 2035, 2121, 17252, 1024, 57...   \n",
            "2  125055.0    208.0  [2326, 1024, 10507, 2226, 2381, 1997, 2556, 73...   \n",
            "3  125055.0    209.0  [2326, 1024, 10507, 2226, 5587, 10497, 2819, 1...   \n",
            "4  152030.0    221.0  [3058, 1997, 4182, 1024, 3348, 1024, 1049, 232...   \n",
            "\n",
            "   Label   chartdate charttime  \n",
            "0    0.0  2192-05-23       NaN  \n",
            "1    1.0  2134-03-09       NaN  \n",
            "2    0.0  2189-03-25       NaN  \n",
            "3    0.0  2189-03-27       NaN  \n",
            "4    1.0  2174-06-03       NaN  \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# dir and function to load raw data\n",
        "train_data_dir = '/content/drive/My Drive/Colab Notebooks/Ecoli/train.csv'\n",
        "\n",
        "def load_processed_data(file_path):\n",
        "  return pd.read_csv(file_path)\n",
        "\n",
        "train_data = load_processed_data(train_data_dir)\n",
        "print(train_data.head())\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "train_data['Label'].value_counts().plot(kind='bar')\n",
        "plt.title('Label Distribution in the Train Dataset')\n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n",
        "  * Model architecture: The model architecture used for this project is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, specifically tailored for clinical notes through modifications known as ClinicalBERT [2]. This adaptation includes the following elements [1]:\n",
        "\n",
        "    * Layer Type and Size: The architecture comprises multiple layers of transformers, each consisting of multi-head attention mechanisms coupled with feed-forward neural networks.\n",
        "    * Activation Function: The transformer layers use the GELU (Gaussian Error Linear Unit) activation function.\n",
        "    * Embedding Layers: The model includes token embeddings, segment embeddings, and position embeddings, all of which are integral to the BERT architecture for handling sequences of tokens.\n",
        "  * Training objectives:\n",
        "      * Loss Function: The primary loss function used is cross-entropy loss, suitable for binary classification tasks.\n",
        "      * Optimizer: The training utilizes the Adam optimizer with weight decay fix.\n",
        "      * Learning Rate: A small learning rate of 1e-07 was used, with a warmup proportion to prevent the model from converging too quickly to a suboptimal solution.\n",
        "  * Others: the base model is pretrained from https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\n",
        "  * I reproduced experiments for three model configurations, adapted from the paper's [repo](https://github.com/zdy93/FTL-Trans):\n",
        "\n",
        "    * ClBERT-am (no embedding) - A Clinical BERT model adjusted mean without position embeddings.\n",
        "    * ClBERT-am (all embedding) - A Clinical BERT model with full position embeddings.\n",
        "    * TL-Trans (all embedding) - Time-LSTM with transformer incorporating all embeddings.\n",
        "\n",
        "Training was performed over 3 epochs to ensure the models could be executed within a reasonable time frame as per reproducibility constraints.\n",
        "\n",
        "I trained the model on my local machine and commented out the training andevaluation code in this notebook. I uploaded the trained TL-Trans model to the Google drive and bellow is a function to load and test it."
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2fcNzPQ0rbg",
        "outputId": "43b35b46-561a-4e99-d194-d3641c8ede03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.100-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.100 (from boto3)\n",
            "  Downloading botocore-1.34.100-py3-none-any.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.100->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.100->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.100->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.100 botocore-1.34.100 jmespath-1.0.1 s3transfer-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuvtGpcS1qfI",
        "outputId": "147f65e9-b4dc-4b42-9a2c-ddc3b18c7c06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_transformers\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/176.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m153.6/176.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.25.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (1.34.100)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (2023.12.25)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from pytorch_transformers) (0.1.99)\n",
            "Collecting sacremoses (from pytorch_transformers)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->pytorch_transformers) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->pytorch_transformers)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.100 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.34.100)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_transformers) (0.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_transformers) (2024.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->pytorch_transformers) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.100->boto3->pytorch_transformers) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->pytorch_transformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->pytorch_transformers) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.100->boto3->pytorch_transformers) (1.16.0)\n",
            "Installing collected packages: sacremoses, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pytorch_transformers-1.2.0 sacremoses-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@author: Dongyu Zhang\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "import six\n",
        "\n",
        "\n",
        "def time_batch_generator(max_len, input_ids, labels, masks, note_ids, chunk_ids, times=None):\n",
        "    \"\"\"batch generator with note_id, chunk_id and time\n",
        "    \"\"\"\n",
        "    size = len(input_ids)\n",
        "    indices = np.arange(size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i < size:\n",
        "            if times is not None:\n",
        "                yield input_ids[indices[i]][-max_len:, :], labels[indices[i]], masks[indices[i]][-max_len:, :], \\\n",
        "                      note_ids[indices[i]][-max_len:], chunk_ids[indices[i]][-max_len:], times[indices[i]][-max_len:]\n",
        "            else:\n",
        "                yield input_ids[indices[i]][-max_len:, :], labels[indices[i]], masks[indices[i]][-max_len:, :], \\\n",
        "                      note_ids[indices[i]][-max_len:], chunk_ids[indices[i]][-max_len:]\n",
        "            i += 1\n",
        "        else:\n",
        "            i = 0\n",
        "            indices = np.arange(size)\n",
        "            np.random.shuffle(indices)\n",
        "            continue\n",
        "\n",
        "\n",
        "def mask_batch_generator(max_len, input_ids, labels, masks):\n",
        "    \"\"\"batch generator\n",
        "    \"\"\"\n",
        "    size = len(input_ids)\n",
        "    indices = np.arange(size)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    i = 0\n",
        "    while True:\n",
        "        if i < size:\n",
        "            yield input_ids[indices[i]][-max_len:, :], labels[indices[i]], masks[indices[i]][-max_len:, :]\n",
        "            i += 1\n",
        "        else:\n",
        "            i = 0\n",
        "            indices = np.arange(size)\n",
        "            np.random.shuffle(indices)\n",
        "            continue\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
        "                  padding='pre', truncating='pre', value=0.):\n",
        "    \"\"\"Pads sequences to the same length.\n",
        "\n",
        "    This function transforms a list of\n",
        "    `num_samples` sequences (lists of integers)\n",
        "    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
        "    `num_timesteps` is either the `maxlen` argument if provided,\n",
        "    or the length of the longest sequence otherwise.\n",
        "\n",
        "    Sequences that are shorter than `num_timesteps`\n",
        "    are padded with `value` at the end.\n",
        "\n",
        "    Sequences longer than `num_timesteps` are truncated\n",
        "    so that they fit the desired length.\n",
        "    The position where padding or truncation happens is determined by\n",
        "    the arguments `padding` and `truncating`, respectively.\n",
        "\n",
        "    Pre-padding is the default.\n",
        "\n",
        "    # Arguments\n",
        "        sequences: List of lists, where each element is a sequence.\n",
        "        maxlen: Int, maximum length of all sequences.\n",
        "        dtype: Type of the output sequences.\n",
        "            To pad sequences with variable length strings, you can use `object`.\n",
        "        padding: String, 'pre' or 'post':\n",
        "            pad either before or after each sequence.\n",
        "        truncating: String, 'pre' or 'post':\n",
        "            remove values from sequences larger than\n",
        "            `maxlen`, either at the beginning or at the end of the sequences.\n",
        "        value: Float or String, padding value.\n",
        "\n",
        "    # Returns\n",
        "        x: Numpy array with shape `(len(sequences), maxlen)`\n",
        "\n",
        "    # Raises\n",
        "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
        "            or in case of invalid shape for a `sequences` entry.\n",
        "    \"\"\"\n",
        "    if not hasattr(sequences, '__len__'):\n",
        "        raise ValueError('`sequences` must be iterable.')\n",
        "    num_samples = len(sequences)\n",
        "\n",
        "    lengths = []\n",
        "    for x in sequences:\n",
        "        try:\n",
        "            lengths.append(len(x))\n",
        "        except TypeError:\n",
        "            raise ValueError('`sequences` must be a list of iterables. '\n",
        "                             'Found non-iterable: ' + str(x))\n",
        "\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
        "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
        "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
        "                         \"You should set `dtype=object` for variable length strings.\"\n",
        "                         .format(dtype, type(value)))\n",
        "\n",
        "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if not len(s):\n",
        "            continue  # empty list/array was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" '\n",
        "                             'not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
        "                             'is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x\n",
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@author: Dongyu Zhang\n",
        "\"\"\"\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import io\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import trange, tqdm\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, \\\n",
        "    auc, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def write_log(content, log_path, print_content=True):\n",
        "    if os.path.exists(log_path):\n",
        "        with open(log_path, 'a') as f:\n",
        "            f.write(\"Time: \" + time.ctime() + \"\\n\")\n",
        "            f.write(content + \"\\n\")\n",
        "            f.write(\"=====================\\n\")\n",
        "    else:\n",
        "        with open(log_path, 'w') as f:\n",
        "            f.write(\"Time: \" + time.ctime() + \"\\n\")\n",
        "            f.write(content + \"\\n\")\n",
        "            f.write(\"=====================\\n\")\n",
        "    if print_content:\n",
        "        print(content)\n",
        "\n",
        "\n",
        "def preprocess1(x):\n",
        "    y = re.sub('\\\\[(.*?)\\\\]', '', x)  # remove de-identified brackets\n",
        "    y = re.sub('[0-9]+\\.', '', y)  # remove 1.2. since the segmenter segments based on this\n",
        "    y = re.sub('dr\\.', 'doctor', y)\n",
        "    y = re.sub('m\\.d\\.', 'md', y)\n",
        "    y = re.sub('admission date:', '', y)\n",
        "    y = re.sub('discharge date:', '', y)\n",
        "    y = re.sub('--|__|==', '', y)\n",
        "    return y\n",
        "\n",
        "\n",
        "def preprocessing(df_less_n, tokenizer):\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].fillna(' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\n', ' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\r', ' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(str.strip)\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.lower()\n",
        "\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
        "\n",
        "    sen = df_less_n['TEXT'].values\n",
        "    tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "    print(\"First sentence tokenized\")\n",
        "    print(tokenized_texts[0])\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    df_less_n['Input_ID'] = input_ids\n",
        "    return df_less_n[['Adm_ID', 'Note_ID', 'TEXT', 'Input_ID', 'Label', 'chartdate', 'charttime']]\n",
        "\n",
        "\n",
        "def word_count_pre(df_less_n):\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].fillna(' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\n', ' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.replace('\\r', ' ')\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(str.strip)\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].str.lower()\n",
        "    df_less_n['TEXT'] = df_less_n['TEXT'].apply(lambda x: preprocess1(x))\n",
        "    return df_less_n\n",
        "\n",
        "\n",
        "def split_into_chunks(df, max_len):\n",
        "    input_ids = df.Input_ID.apply(lambda x: x[1:-1].replace(' ', '').split(','))\n",
        "    df_len = len(df)\n",
        "    Adm_ID, Note_ID, Input_ID, Label, chartdate, charttime = [], [], [], [], [], []\n",
        "    for i in tqdm(range(df_len)):\n",
        "        x = input_ids[i]\n",
        "        n = int(len(x) / (max_len - 2))\n",
        "        for j in range(n):\n",
        "            Adm_ID.append(df.Adm_ID[i])\n",
        "            Note_ID.append(df.Note_ID[i])\n",
        "            sub_ids = x[j * (max_len - 2): (j + 1) * (max_len - 2)]\n",
        "            sub_ids.insert(0, '101')\n",
        "            sub_ids.append('102')\n",
        "            Input_ID.append(' '.join(sub_ids))\n",
        "            Label.append(df.Label[i])\n",
        "            chartdate.append(df.chartdate[i])\n",
        "            charttime.append(df.charttime[i])\n",
        "        if len(x) % (max_len - 2) > 10:\n",
        "            Adm_ID.append(df.Adm_ID[i])\n",
        "            Note_ID.append(df.Note_ID[i])\n",
        "            sub_ids = x[-((len(x)) % (max_len - 2)):]\n",
        "            sub_ids.insert(0, '101')\n",
        "            sub_ids.append('102')\n",
        "            Input_ID.append(' '.join(sub_ids))\n",
        "            Label.append(df.Label[i])\n",
        "            chartdate.append(df.chartdate[i])\n",
        "            charttime.append(df.charttime[i])\n",
        "    new_df = pd.DataFrame({'Adm_ID': Adm_ID,\n",
        "                           'Note_ID': Note_ID,\n",
        "                           'Input_ID': Input_ID,\n",
        "                           'Label': Label,\n",
        "                           'chartdate': chartdate,\n",
        "                           'charttime': charttime})\n",
        "    new_df = new_df.astype({'Adm_ID': 'int64', 'Note_ID': 'int64', 'Label': 'int64'})\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def Tokenize(df, max_length, tokenizer):\n",
        "    labels = df.Label.values\n",
        "    if 'TEXT' in df.columns:\n",
        "        sen = df.TEXT.values\n",
        "        labels = df.Label.values\n",
        "        sen = [\"[CLS] \" + x + \" [SEP]\" for x in sen]\n",
        "        tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "        print(\"First sentence tokenized\")\n",
        "        print(tokenized_texts[0])\n",
        "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    else:\n",
        "        assert 'Input_ID' in df.columns\n",
        "        input_ids = df.Input_ID.apply(lambda x: x[1:-1].split(', '))\n",
        "        input_ids = input_ids.apply(lambda x: [int(i) for i in x])\n",
        "        input_ids = input_ids.values\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return labels, input_ids, attention_masks\n",
        "\n",
        "\n",
        "def Tokenize_with_note_id(df, max_length, tokenizer):\n",
        "    labels = df.Label.values\n",
        "    note_ids = df.Note_ID.values\n",
        "    if 'TEXT' in df.columns:\n",
        "        sen = df.TEXT.values\n",
        "        labels = df.Label.values\n",
        "        sen = [\"[CLS] \" + x + \" [SEP]\" for x in sen]\n",
        "        tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "        print(\"First sentence tokenized\")\n",
        "        print(tokenized_texts[0])\n",
        "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    else:\n",
        "        assert 'Input_ID' in df.columns\n",
        "        input_ids = df.Input_ID.apply(lambda x: x[1:-1].split(', '))\n",
        "        input_ids = input_ids.apply(lambda x: [int(i) for i in x])\n",
        "        input_ids = input_ids.values\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return labels, input_ids, attention_masks, note_ids\n",
        "\n",
        "\n",
        "def Tokenize_with_note_id_time(df, max_length, tokenizer):\n",
        "    labels = df.Label.values\n",
        "    note_ids = df.Note_ID.values\n",
        "    times = pd.to_datetime(df.chartdate.values)\n",
        "    times = times - times.min()\n",
        "    times = times.days.values\n",
        "    if 'TEXT' in df.columns:\n",
        "        sen = df.TEXT.values\n",
        "        labels = df.Label.values\n",
        "        sen = [\"[CLS] \" + x + \" [SEP]\" for x in sen]\n",
        "        tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "        print(\"First sentence tokenized\")\n",
        "        print(tokenized_texts[0])\n",
        "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    else:\n",
        "        assert 'Input_ID' in df.columns\n",
        "        input_ids = df.Input_ID.apply(lambda x: x.split(' '))\n",
        "        input_ids = input_ids.apply(lambda x: [int(i) for i in x])\n",
        "        input_ids = input_ids.values\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return labels, input_ids, attention_masks, note_ids, times\n",
        "\n",
        "\n",
        "def Tokenize_with_note_id_hour(df, max_length, tokenizer):\n",
        "    labels = df.Label.values\n",
        "    note_ids = df.Note_ID.values\n",
        "    times = pd.to_datetime(df.charttime.values)\n",
        "    times = times - times.min()\n",
        "    times = times / pd.Timedelta(days=1)\n",
        "    if 'TEXT' in df.columns:\n",
        "        sen = df.TEXT.values\n",
        "        labels = df.Label.values\n",
        "        sen = [\"[CLS] \" + x + \" [SEP]\" for x in sen]\n",
        "        tokenized_texts = [tokenizer.tokenize(x) for x in sen]\n",
        "        print(\"First sentence tokenized\")\n",
        "        print(tokenized_texts[0])\n",
        "        input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    else:\n",
        "        assert 'Input_ID' in df.columns\n",
        "        input_ids = df.Input_ID.apply(lambda x: x[1:-1].split(', '))\n",
        "        input_ids = input_ids.apply(lambda x: [int(i) for i in x])\n",
        "        input_ids = input_ids.values\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return labels, input_ids, attention_masks, note_ids, times\n",
        "\n",
        "\n",
        "def reorder_by_time(data):\n",
        "    data.chartdate = pd.to_datetime(data.chartdate)\n",
        "    data.charttime = pd.to_datetime(data.charttime)\n",
        "    data.loc[data.charttime.isna(), 'charttime'] = data[data.charttime.isna()].chartdate + pd.Timedelta(hours=23,\n",
        "                                                                                                        minutes=59,\n",
        "                                                                                                        seconds=59)\n",
        "    data = data.sort_values(by=['Adm_ID', 'charttime', 'Note_ID'])\n",
        "    data.reset_index(inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "def concat_by_id_list(df, labels, inputs, masks, str_len):\n",
        "    final_labels, final_inputs, final_masks = [], [], []\n",
        "    id_lists = df.Adm_ID.unique()\n",
        "    for id in id_lists:\n",
        "        id_ix = df.index[df.Adm_ID == id].to_list()\n",
        "        final_inputs.append(inputs[id_ix])\n",
        "        final_masks.append(masks[id_ix])\n",
        "        final_labels.append(labels[id_ix].max())\n",
        "    return final_labels, final_inputs, final_masks, id_lists\n",
        "\n",
        "\n",
        "def concat_by_id_list_with_note_chunk_id(df, labels, inputs, masks, note_ids, str_len):\n",
        "    final_labels, final_inputs, final_masks, final_note_ids, final_chunk_ids = [], [], [], [], []\n",
        "    id_lists = df.Adm_ID.unique()\n",
        "    for id in id_lists:\n",
        "        id_ix = df.index[df.Adm_ID == id].to_list()\n",
        "        final_inputs.append(inputs[id_ix])\n",
        "        final_masks.append(masks[id_ix])\n",
        "        final_labels.append(labels[id_ix].max())\n",
        "        final_note_ids.append(note_ids[id_ix])\n",
        "        final_chunk_ids.append(torch.tensor(list(range(len(id_ix)))[::-1]))\n",
        "    return final_labels, final_inputs, final_masks, id_lists, final_note_ids, final_chunk_ids\n",
        "\n",
        "\n",
        "def concat_by_id_list_with_note_chunk_id_time(df, labels, inputs, masks, note_ids, times, str_len):\n",
        "    final_labels, final_inputs, final_masks, final_note_ids, final_chunk_ids, final_times = [], [], [], [], [], []\n",
        "    id_lists = df.Adm_ID.unique()\n",
        "    for id in id_lists:\n",
        "        id_ix = df.index[df.Adm_ID == id].to_list()\n",
        "        final_inputs.append(inputs[id_ix])\n",
        "        final_masks.append(masks[id_ix])\n",
        "        final_labels.append(labels[id_ix].max())\n",
        "        final_note_ids.append(note_ids[id_ix])\n",
        "        final_chunk_ids.append(torch.tensor(list(range(len(id_ix)))[::-1]))\n",
        "        final_times.append(torch.tensor(np.concatenate([np.zeros(1), np.diff(times[id_ix])])))\n",
        "    return final_labels, final_inputs, final_masks, id_lists, final_note_ids, final_chunk_ids, final_times\n",
        "\n",
        "\n",
        "def convert_note_ids(note_ids):\n",
        "    new_dict = dict(zip(pd.Series(note_ids).unique(), range(len(pd.Series(note_ids).unique()))[::-1]))\n",
        "    new_ids = [new_dict[i] for i in note_ids]\n",
        "    return torch.tensor(new_ids)\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.asarray([1 if i else 0 for i in (preds.flatten() >= 0.5)])\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "def model_auc(y_true, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    return auc_score, fpr, tpr, thresholds\n",
        "\n",
        "\n",
        "def model_aupr(y_true, y_pred):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    aupr_score = auc(recall, precision)\n",
        "    return aupr_score, precision, recall, thresholds\n",
        "\n",
        "\n",
        "def write_performance(flat_true_labels, flat_predictions, flat_logits, config):\n",
        "    test_accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "    test_f1 = f1_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_prec = precision_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_rec = recall_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_auc, _, _, _ = model_auc(flat_true_labels, flat_logits)\n",
        "\n",
        "    test_mc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "    test_aupr, _, _, _ = model_aupr(flat_true_labels, flat_logits)\n",
        "\n",
        "    test_msl = 64\n",
        "\n",
        "    test_seed = 42\n",
        "\n",
        "    test_dir_code = \"./Ecoli\"\n",
        "\n",
        "    test_time = time.ctime()\n",
        "\n",
        "    exp_path = \"{}_{}_{}.csv\".format(config.task_name, config.embed_mode, test_msl)\n",
        "\n",
        "    header = \"Len,Dir,Seed,Accuracy,F1_Score,Precision,Recall,AUC,MCC,AUPR,Time\"\n",
        "    content = \"{},{},{},{},{},{},{},{},{},{},{}\".format(test_msl,\n",
        "                                                        test_dir_code,\n",
        "                                                        test_seed,\n",
        "                                                        test_accuracy,\n",
        "                                                        test_f1,\n",
        "                                                        test_prec,\n",
        "                                                        test_rec,\n",
        "                                                        test_auc,\n",
        "                                                        test_mc,\n",
        "                                                        test_aupr,\n",
        "                                                        test_time)\n",
        "\n",
        "    print(\"Test Patient Level Accuracy: {}\\n\"\n",
        "              \"Test Patient Level F1 Score: {}\\n\"\n",
        "              \"Test Patient Level Precision: {}\\n\"\n",
        "              \"Test Patient Level Recall: {}\\n\"\n",
        "              \"Test Patient Level AUC: {} \\n\"\n",
        "              \"Test Patient Level Matthew's correlation coefficient: {}\\n\"\n",
        "              \"Test Patient Level AUPR: {} \\n\"\n",
        "              \"All Finished!\".format(test_accuracy,\n",
        "                                     test_f1,\n",
        "                                     test_prec,\n",
        "                                     test_rec,\n",
        "                                     test_auc,\n",
        "                                     test_mc,\n",
        "                                     test_aupr))\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Utilities for working with the local dataset cache.\n",
        "This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\n",
        "Copyright by the AllenNLP authors.\n",
        "\"\"\"\n",
        "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "from functools import wraps\n",
        "from hashlib import sha256\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "import boto3\n",
        "import requests\n",
        "from botocore.exceptions import ClientError\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    from urllib.parse import urlparse\n",
        "except ImportError:\n",
        "    from urlparse import urlparse\n",
        "\n",
        "try:\n",
        "    from pathlib import Path\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                                   Path.home() / '.pytorch_pretrained_bert'))\n",
        "except (AttributeError, ImportError):\n",
        "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
        "                                              os.path.join(os.path.expanduser(\"~\"), '.pytorch_pretrained_bert'))\n",
        "\n",
        "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
        "\n",
        "\n",
        "def url_to_filename(url, etag=None):\n",
        "    \"\"\"\n",
        "    Convert `url` into a hashed filename in a repeatable way.\n",
        "    If `etag` is specified, append its hash to the url's, delimited\n",
        "    by a period.\n",
        "    \"\"\"\n",
        "    url_bytes = url.encode('utf-8')\n",
        "    url_hash = sha256(url_bytes)\n",
        "    filename = url_hash.hexdigest()\n",
        "\n",
        "    if etag:\n",
        "        etag_bytes = etag.encode('utf-8')\n",
        "        etag_hash = sha256(etag_bytes)\n",
        "        filename += '.' + etag_hash.hexdigest()\n",
        "\n",
        "    return filename\n",
        "\n",
        "\n",
        "def filename_to_url(filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
        "    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "    if not os.path.exists(cache_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
        "\n",
        "    meta_path = cache_path + '.json'\n",
        "    if not os.path.exists(meta_path):\n",
        "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
        "\n",
        "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
        "        metadata = json.load(meta_file)\n",
        "    url = metadata['url']\n",
        "    etag = metadata['etag']\n",
        "\n",
        "    return url, etag\n",
        "\n",
        "\n",
        "def cached_path(url_or_filename, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given something that might be a URL (or might be a local path),\n",
        "    determine which. If it's a URL, download the file and cache it, and\n",
        "    return the path to the cached file. If it's already a local path,\n",
        "    make sure the file exists and then return the path.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n",
        "        url_or_filename = str(url_or_filename)\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    parsed = urlparse(url_or_filename)\n",
        "\n",
        "    if parsed.scheme in ('http', 'https', 's3'):\n",
        "        # URL, so get it from the cache (downloading if necessary)\n",
        "        return get_from_cache(url_or_filename, cache_dir)\n",
        "    elif os.path.exists(url_or_filename):\n",
        "        # File, and it exists.\n",
        "        return url_or_filename\n",
        "    elif parsed.scheme == '':\n",
        "        # File, but it doesn't exist.\n",
        "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
        "    else:\n",
        "        # Something unknown\n",
        "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
        "\n",
        "\n",
        "def split_s3_path(url):\n",
        "    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    if not parsed.netloc or not parsed.path:\n",
        "        raise ValueError(\"bad s3 path {}\".format(url))\n",
        "    bucket_name = parsed.netloc\n",
        "    s3_path = parsed.path\n",
        "    # Remove '/' at beginning of path.\n",
        "    if s3_path.startswith(\"/\"):\n",
        "        s3_path = s3_path[1:]\n",
        "    return bucket_name, s3_path\n",
        "\n",
        "\n",
        "def s3_request(func):\n",
        "    \"\"\"\n",
        "    Wrapper function for s3 requests in order to create more helpful error\n",
        "    messages.\n",
        "    \"\"\"\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(url, *args, **kwargs):\n",
        "        try:\n",
        "            return func(url, *args, **kwargs)\n",
        "        except ClientError as exc:\n",
        "            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n",
        "                raise EnvironmentError(\"file {} not found\".format(url))\n",
        "            else:\n",
        "                raise\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_etag(url):\n",
        "    \"\"\"Check ETag on S3 object.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_object = s3_resource.Object(bucket_name, s3_path)\n",
        "    return s3_object.e_tag\n",
        "\n",
        "\n",
        "@s3_request\n",
        "def s3_get(url, temp_file):\n",
        "    \"\"\"Pull a file directly from S3.\"\"\"\n",
        "    s3_resource = boto3.resource(\"s3\")\n",
        "    bucket_name, s3_path = split_s3_path(url)\n",
        "    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n",
        "\n",
        "\n",
        "def http_get(url, temp_file):\n",
        "    req = requests.get(url, stream=True)\n",
        "    content_length = req.headers.get('Content-Length')\n",
        "    total = int(content_length) if content_length is not None else None\n",
        "    progress = tqdm(unit=\"B\", total=total)\n",
        "    for chunk in req.iter_content(chunk_size=1024):\n",
        "        if chunk: # filter out keep-alive new chunks\n",
        "            progress.update(len(chunk))\n",
        "            temp_file.write(chunk)\n",
        "    progress.close()\n",
        "\n",
        "\n",
        "def get_from_cache(url, cache_dir=None):\n",
        "    \"\"\"\n",
        "    Given a URL, look for the corresponding dataset in the local cache.\n",
        "    If it's not there, download it. Then return the path to the cached file.\n",
        "    \"\"\"\n",
        "    if cache_dir is None:\n",
        "        cache_dir = PYTORCH_PRETRAINED_BERT_CACHE\n",
        "    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n",
        "        cache_dir = str(cache_dir)\n",
        "\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    # Get eTag to add to filename, if it exists.\n",
        "    if url.startswith(\"s3://\"):\n",
        "        etag = s3_etag(url)\n",
        "    else:\n",
        "        response = requests.head(url, allow_redirects=True)\n",
        "        if response.status_code != 200:\n",
        "            raise IOError(\"HEAD request failed for url {} with status code {}\"\n",
        "                          .format(url, response.status_code))\n",
        "        etag = response.headers.get(\"ETag\")\n",
        "\n",
        "    filename = url_to_filename(url, etag)\n",
        "\n",
        "    # get cache path to put the file\n",
        "    cache_path = os.path.join(cache_dir, filename)\n",
        "\n",
        "    if not os.path.exists(cache_path):\n",
        "        # Download to temporary file, then copy to cache dir once finished.\n",
        "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
        "        with tempfile.NamedTemporaryFile() as temp_file:\n",
        "            logger.info(\"%s not found in cache, downloading to %s\", url, temp_file.name)\n",
        "\n",
        "            # GET file object\n",
        "            if url.startswith(\"s3://\"):\n",
        "                s3_get(url, temp_file)\n",
        "            else:\n",
        "                http_get(url, temp_file)\n",
        "\n",
        "            # we are copying the file before closing it, so flush to avoid truncation\n",
        "            temp_file.flush()\n",
        "            # shutil.copyfileobj() starts at the current position, so go to the start\n",
        "            temp_file.seek(0)\n",
        "\n",
        "            logger.info(\"copying %s to cache at %s\", temp_file.name, cache_path)\n",
        "            with open(cache_path, 'wb') as cache_file:\n",
        "                shutil.copyfileobj(temp_file, cache_file)\n",
        "\n",
        "            logger.info(\"creating metadata file for %s\", cache_path)\n",
        "            meta = {'url': url, 'etag': etag}\n",
        "            meta_path = cache_path + '.json'\n",
        "            with open(meta_path, 'w', encoding=\"utf-8\") as meta_file:\n",
        "                json.dump(meta, meta_file)\n",
        "\n",
        "            logger.info(\"removing temp file %s\", temp_file.name)\n",
        "\n",
        "    return cache_path\n",
        "\n",
        "\n",
        "def read_set_from_file(filename):\n",
        "    '''\n",
        "    Extract a de-duped collection (set) of text from a file.\n",
        "    Expected file format is one item per line.\n",
        "    '''\n",
        "    collection = set()\n",
        "    with open(filename, 'r', encoding='utf-8') as file_:\n",
        "        for line in file_:\n",
        "            collection.add(line.rstrip())\n",
        "    return collection\n",
        "\n",
        "\n",
        "def get_file_extension(path, dot=True, lower=True):\n",
        "    ext = os.path.splitext(path)[1]\n",
        "    ext = ext if dot else ext[1:]\n",
        "    return ext.lower() if lower else ext"
      ],
      "metadata": {
        "id": "3WY8X1gdYPlx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model. \"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "BERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin\",\n",
        "    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin\",\n",
        "}\n",
        "\n",
        "BERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json\",\n",
        "    'bert-base-german-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json\",\n",
        "    'bert-large-cased-whole-word-masking': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json\",\n",
        "    'bert-large-uncased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-large-cased-whole-word-masking-finetuned-squad': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json\",\n",
        "    'bert-base-cased-finetuned-mrpc': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json\",\n",
        "}\n",
        "\n",
        "\n",
        "def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n",
        "    \"\"\" Load tf checkpoints in a pytorch model.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import re\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "    except ImportError:\n",
        "        logger.error(\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n",
        "            \"https://www.tensorflow.org/install/ for installation instructions.\")\n",
        "        raise\n",
        "    tf_path = os.path.abspath(tf_checkpoint_path)\n",
        "    logger.info(\"Converting TensorFlow checkpoint from {}\".format(tf_path))\n",
        "    # Load weights from TF model\n",
        "    init_vars = tf.train.list_variables(tf_path)\n",
        "    names = []\n",
        "    arrays = []\n",
        "    for name, shape in init_vars:\n",
        "        logger.info(\"Loading TF weight {} with shape {}\".format(name, shape))\n",
        "        array = tf.train.load_variable(tf_path, name)\n",
        "        names.append(name)\n",
        "        arrays.append(array)\n",
        "\n",
        "    for name, array in zip(names, arrays):\n",
        "        name = name.split('/')\n",
        "        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n",
        "        # which are not required for using pretrained model\n",
        "        if any(n in [\"adam_v\", \"adam_m\", \"global_step\"] for n in name):\n",
        "            logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "            continue\n",
        "        pointer = model\n",
        "        for m_name in name:\n",
        "            if re.fullmatch(r'[A-Za-z]+_\\d+', m_name):\n",
        "                l = re.split(r'_(\\d+)', m_name)\n",
        "            else:\n",
        "                l = [m_name]\n",
        "            if l[0] == 'kernel' or l[0] == 'gamma':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'output_bias' or l[0] == 'beta':\n",
        "                pointer = getattr(pointer, 'bias')\n",
        "            elif l[0] == 'output_weights':\n",
        "                pointer = getattr(pointer, 'weight')\n",
        "            elif l[0] == 'squad':\n",
        "                pointer = getattr(pointer, 'classifier')\n",
        "            else:\n",
        "                try:\n",
        "                    pointer = getattr(pointer, l[0])\n",
        "                except AttributeError:\n",
        "                    logger.info(\"Skipping {}\".format(\"/\".join(name)))\n",
        "                    continue\n",
        "            if len(l) >= 2:\n",
        "                num = int(l[1])\n",
        "                pointer = pointer[num]\n",
        "        if m_name[-11:] == '_embeddings':\n",
        "            pointer = getattr(pointer, 'weight')\n",
        "        elif m_name == 'kernel':\n",
        "            array = np.transpose(array)\n",
        "        try:\n",
        "            assert pointer.shape == array.shape\n",
        "        except AssertionError as e:\n",
        "            e.args += (pointer.shape, array.shape)\n",
        "            raise\n",
        "        logger.info(\"Initialize PyTorch weight {}\".format(name))\n",
        "        pointer.data = torch.from_numpy(array)\n",
        "    return model\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class PretrainedConfig(object):\n",
        "    r\"\"\" Base class for all configuration classes.\n",
        "        Handles a few parameters common to all models' configurations as well as methods for loading/downloading/saving configurations.\n",
        "\n",
        "        Note:\n",
        "            A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to initialize a model does **not** load the model weights.\n",
        "            It only affects the model's configuration.\n",
        "\n",
        "        Class attributes (overridden by derived classes):\n",
        "            - ``pretrained_config_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained model configurations as values.\n",
        "\n",
        "        Parameters:\n",
        "            ``finetuning_task``: string, default `None`. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n",
        "            ``num_labels``: integer, default `2`. Number of classes to use when the model is a classification model (sequences/tokens)\n",
        "            ``output_attentions``: boolean, default `False`. Should the model returns attentions weights.\n",
        "            ``output_hidden_states``: string, default `False`. Should the model returns all hidden-states.\n",
        "            ``torchscript``: string, default `False`. Is the model used with Torchscript.\n",
        "    \"\"\"\n",
        "    pretrained_config_archive_map = {}\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.finetuning_task = kwargs.pop('finetuning_task', None)\n",
        "        self.num_labels = kwargs.pop('num_labels', 2)\n",
        "        self.output_attentions = kwargs.pop('output_attentions', False)\n",
        "        self.output_hidden_states = kwargs.pop('output_hidden_states', False)\n",
        "        self.torchscript = kwargs.pop('torchscript', False)\n",
        "        self.pruned_heads = kwargs.pop('pruned_heads', {})\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\" Save a configuration object to the directory `save_directory`, so that it\n",
        "            can be re-loaded using the :func:`~pytorch_transformers.PretrainedConfig.from_pretrained` class method.\n",
        "        \"\"\"\n",
        "        assert os.path.isdir(save_directory), \"Saving path should be a directory where the model and configuration can be saved\"\n",
        "\n",
        "        # If we save using the predefined names, we can load using `from_pretrained`\n",
        "        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n",
        "\n",
        "        self.to_json_file(output_config_file)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n",
        "        r\"\"\" Instantiate a :class:`~pytorch_transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n",
        "\n",
        "        Parameters:\n",
        "            pretrained_model_name_or_path: either:\n",
        "\n",
        "                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n",
        "                - a path to a `directory` containing a configuration file saved using the :func:`~pytorch_transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
        "                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n",
        "\n",
        "            cache_dir: (`optional`) string:\n",
        "                Path to a directory in which a downloaded pre-trained model\n",
        "                configuration should be cached if the standard cache should not be used.\n",
        "\n",
        "            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n",
        "\n",
        "                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n",
        "                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n",
        "\n",
        "            force_download: (`optional`) boolean, default False:\n",
        "                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n",
        "\n",
        "            proxies: (`optional`) dict, default None:\n",
        "                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
        "                The proxies are used on each request.\n",
        "\n",
        "            return_unused_kwargs: (`optional`) bool:\n",
        "\n",
        "                - If False, then this function returns just the final configuration object.\n",
        "                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n",
        "            # derived class: BertConfig\n",
        "            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n",
        "            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n",
        "            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n",
        "            config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n",
        "            assert config.output_attention == True\n",
        "            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n",
        "                                                               foo=False, return_unused_kwargs=True)\n",
        "            assert config.output_attention == True\n",
        "            assert unused_kwargs == {'foo': False}\n",
        "\n",
        "        \"\"\"\n",
        "        cache_dir = kwargs.pop('cache_dir', None)\n",
        "        force_download = kwargs.pop('force_download', False)\n",
        "        proxies = kwargs.pop('proxies', None)\n",
        "        return_unused_kwargs = kwargs.pop('return_unused_kwargs', False)\n",
        "\n",
        "        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n",
        "            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\n",
        "        elif os.path.isdir(pretrained_model_name_or_path):\n",
        "            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n",
        "        else:\n",
        "            config_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_config_file = cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n",
        "        except EnvironmentError as e:\n",
        "            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n",
        "                logger.error(\n",
        "                    \"Couldn't reach server at '{}' to download pretrained model configuration file.\".format(\n",
        "                        config_file))\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"Model name '{}' was not found in model name list ({}). \"\n",
        "                    \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                    \"associated to this path or url.\".format(\n",
        "                        pretrained_model_name_or_path,\n",
        "                        ', '.join(cls.pretrained_config_archive_map.keys()),\n",
        "                        config_file))\n",
        "            raise e\n",
        "        if resolved_config_file == config_file:\n",
        "            logger.info(\"loading configuration file {}\".format(config_file))\n",
        "        else:\n",
        "            logger.info(\"loading configuration file {} from cache at {}\".format(\n",
        "                config_file, resolved_config_file))\n",
        "\n",
        "        # Load config\n",
        "        config = cls.from_json_file(resolved_config_file)\n",
        "\n",
        "        if hasattr(config, 'pruned_heads'):\n",
        "            config.pruned_heads = dict((int(key), set(value)) for key, value in config.pruned_heads.items())\n",
        "\n",
        "        # Update config with kwargs if needed\n",
        "        to_remove = []\n",
        "        for key, value in kwargs.items():\n",
        "            if hasattr(config, key):\n",
        "                setattr(config, key, value)\n",
        "                to_remove.append(key)\n",
        "        for key in to_remove:\n",
        "            kwargs.pop(key, None)\n",
        "\n",
        "        logger.info(\"Model config %s\", config)\n",
        "        if return_unused_kwargs:\n",
        "            return config, kwargs\n",
        "        else:\n",
        "            return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `Config` from a Python dictionary of parameters.\"\"\"\n",
        "        config = cls(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.__dict__ == other.__dict__\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "    def to_json_file(self, json_file_path):\n",
        "        \"\"\" Save this instance to a json file.\"\"\"\n",
        "        with open(json_file_path, \"w\", encoding='utf-8') as writer:\n",
        "            writer.write(self.to_json_string())\n",
        "\n",
        "\n",
        "class BertConfig(PretrainedConfig):\n",
        "    r\"\"\"\n",
        "        :class:`~pytorch_transformers.BertConfig` is the configuration class to store the configuration of a\n",
        "        `BertModel`.\n",
        "\n",
        "\n",
        "        Arguments:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "            layer_norm_eps: The epsilon used by LayerNorm.\n",
        "    \"\"\"\n",
        "    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file=30522,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02,\n",
        "                 layer_norm_eps=1e-12,\n",
        "                 **kwargs):\n",
        "        super(BertConfig, self).__init__(**kwargs)\n",
        "        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n",
        "                        and isinstance(vocab_size_or_config_json_file, unicode)):\n",
        "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "            self.layer_norm_eps = layer_norm_eps\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \" or the path to a pretrained model config file (str)\")\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "    from apex.normalization.fused_layer_norm import FusedLayerNorm as BertLayerNorm\n",
        "except (ImportError, AttributeError) as e:\n",
        "    logger.info(\"Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\")\n",
        "    BertLayerNorm = torch.nn.LayerNorm\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        if position_ids is None:\n",
        "            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.output_attentions = config.output_attentions\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n",
        "        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n",
        "        for head in heads:\n",
        "            # Compute how many pruned heads are before the head and move the index accordingly\n",
        "            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n",
        "            mask[head] = 0\n",
        "        mask = mask.view(-1).contiguous().eq(1)\n",
        "        index = torch.arange(len(mask))[mask].long()\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask, head_mask=None):\n",
        "        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n",
        "        attention_output = self.output(self_outputs[0], input_tensor)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        self.output_attentions = config.output_attentions\n",
        "        self.output_hidden_states = config.output_hidden_states\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, head_mask=None):\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if self.output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if self.output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if self.output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if self.output_hidden_states:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if self.output_attentions:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(config.hidden_size,\n",
        "                                 config.vocab_size,\n",
        "                                 bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class PreTrainedBertModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedBertModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.beta.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            module.gamma.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "\n",
        "        Params:\n",
        "            pretrained_model_name: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-base-multilingual`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    pretrained_model_name))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file):\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "        state_dict = torch.load(weights_path, map_location = 'cpu')\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        return model\n",
        "\n",
        "\n",
        "class PreTrainedBertModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedBertModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.beta.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            module.gamma.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "\n",
        "        Params:\n",
        "            pretrained_model_name: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-base-multilingual`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    pretrained_model_name))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file):\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "        state_dict = torch.load(weights_path, map_location = 'cpu')\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        return model\n",
        "\n",
        "\n",
        "class BertModel(PreTrainedBertModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block,\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForPreTraining(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n",
        "            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n",
        "            in ``[0, ..., config.vocab_size]``\n",
        "        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n",
        "            Indices should be in ``[0, 1]``.\n",
        "            ``0`` indicates sequence B is a continuation of sequence A,\n",
        "            ``1`` indicates sequence B is a random sequence.\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n",
        "        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids)\n",
        "        prediction_scores, seq_relationship_scores = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForPreTraining, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                   self.bert.embeddings.word_embeddings)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n",
        "                next_sentence_label=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForMaskedLM(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the masked language modeling loss.\n",
        "            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n",
        "            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n",
        "            in ``[0, ..., config.vocab_size]``\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Masked language modeling loss.\n",
        "        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n",
        "            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, masked_lm_labels=input_ids)\n",
        "        loss, prediction_scores = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMaskedLM, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        \"\"\" Make sure we are sharing the input and output embeddings.\n",
        "            Export to TorchScript can't handle parameter sharing so we are cloning them instead.\n",
        "        \"\"\"\n",
        "        self._tie_or_clone_weights(self.cls.predictions.decoder,\n",
        "                                   self.bert.embeddings.word_embeddings)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
        "        if masked_lm_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n",
        "            outputs = (masked_lm_loss,) + outputs\n",
        "\n",
        "        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForNextSentencePrediction(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n",
        "            Indices should be in ``[0, 1]``.\n",
        "            ``0`` indicates sequence B is a continuation of sequence A,\n",
        "            ``1`` indicates sequence B is a random sequence.\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Next sequence prediction (classification) loss.\n",
        "        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n",
        "            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids)\n",
        "        seq_relationship_scores = outputs[0]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForNextSentencePrediction, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        seq_relationship_score = self.cls(pooled_output)\n",
        "\n",
        "        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n",
        "            outputs = (next_sentence_loss,) + outputs\n",
        "\n",
        "        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForSequenceClassification(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
        "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForMultipleChoice(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "    Inputs:\n",
        "        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n",
        "\n",
        "            (a) For sequence pairs:\n",
        "\n",
        "                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n",
        "\n",
        "                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n",
        "\n",
        "            (b) For single sequences:\n",
        "\n",
        "                ``tokens:         [CLS] the dog is hairy . [SEP]``\n",
        "\n",
        "                ``token_type_ids:   0   0   0   0  0     0   0``\n",
        "\n",
        "            Indices can be obtained using :class:`pytorch_transformers.BertTokenizer`.\n",
        "            See :func:`pytorch_transformers.PreTrainedTokenizer.encode` and\n",
        "            :func:`pytorch_transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n",
        "        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Segment token indices to indicate first and second portions of the inputs.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n",
        "            corresponds to a `sentence B` token\n",
        "            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n",
        "        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n",
        "            Mask to avoid performing attention on padding token indices.\n",
        "            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n",
        "        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n",
        "            Mask to nullify selected heads of the self-attention modules.\n",
        "            Mask values selected in ``[0, 1]``:\n",
        "            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for computing the multiple choice classification loss.\n",
        "            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n",
        "            of the input tensors. (see `input_ids` above)\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss.\n",
        "        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n",
        "            of the input tensors. (see `input_ids` above).\n",
        "            Classification scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForMultipleChoice.from_pretrained('bert-base-uncased')\n",
        "        choices = [\"Hello, my dog is cute\", \"Hello, my cat is amazing\"]\n",
        "        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n",
        "        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, classification_scores = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMultipleChoice, self).__init__(config)\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        num_choices = input_ids.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        outputs = self.bert(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids,\n",
        "                            attention_mask=flat_attention_mask, head_mask=head_mask)\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForTokenClassification(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n",
        "            Labels for computing the token classification loss.\n",
        "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss.\n",
        "        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n",
        "            Classification scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, scores = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForTokenClassification, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)[active_loss]\n",
        "                active_labels = labels.view(-1)[active_loss]\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), scores, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "\n",
        "class BertForQuestionAnswering(PreTrainedBertModel):\n",
        "    r\"\"\"\n",
        "        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
        "            Position outside of the sequence are not taken into account for computing the loss.\n",
        "        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (`sequence_length`).\n",
        "            Position outside of the sequence are not taken into account for computing the loss.\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n",
        "        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
        "            Span-start scores (before SoftMax).\n",
        "        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n",
        "            Span-end scores (before SoftMax).\n",
        "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
        "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
        "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
        "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
        "        start_positions = torch.tensor([1])\n",
        "        end_positions = torch.tensor([3])\n",
        "        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss, start_scores, end_scores = outputs[:2]\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,\n",
        "                end_positions=None, position_ids=None, head_mask=None):\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import (absolute_import, division, print_function,\n",
        "                        unicode_literals)\n",
        "\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from io import open\n",
        "\n",
        "import six\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "TF_WEIGHTS_NAME = 'model.ckpt'\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.nn import Identity\n",
        "except ImportError:\n",
        "    # Older PyTorch compatibility\n",
        "    class Identity(nn.Module):\n",
        "        r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
        "        \"\"\"\n",
        "        def __init__(self, *args, **kwargs):\n",
        "            super(Identity, self).__init__()\n",
        "\n",
        "        def forward(self, input):\n",
        "            return input\n",
        "\n",
        "\n",
        "if not six.PY2:\n",
        "    def add_start_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            fn.__doc__ = ''.join(docstr) + fn.__doc__\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "\n",
        "    def add_end_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            fn.__doc__ = fn.__doc__ + ''.join(docstr)\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "else:\n",
        "    # Not possible to update class docstrings on python2\n",
        "    def add_start_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "\n",
        "    def add_end_docstrings(*docstr):\n",
        "        def docstring_decorator(fn):\n",
        "            return fn\n",
        "        return docstring_decorator\n",
        "\n",
        "\n",
        "class Conv1D(nn.Module):\n",
        "    def __init__(self, nf, nx):\n",
        "        \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n",
        "            Basically works like a Linear layer but the weights are transposed\n",
        "        \"\"\"\n",
        "        super(Conv1D, self).__init__()\n",
        "        self.nf = nf\n",
        "        w = torch.empty(nx, nf)\n",
        "        nn.init.normal_(w, std=0.02)\n",
        "        self.weight = nn.Parameter(w)\n",
        "        self.bias = nn.Parameter(torch.zeros(nf))\n",
        "\n",
        "    def forward(self, x):\n",
        "        size_out = x.size()[:-1] + (self.nf,)\n",
        "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
        "        x = x.view(*size_out)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerStartLogits(nn.Module):\n",
        "    \"\"\" Compute SQuAD start_logits from sequence hidden states. \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerStartLogits, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states, p_mask=None):\n",
        "        \"\"\" Args:\n",
        "            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n",
        "                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n",
        "                1.0 means token should be masked.\n",
        "        \"\"\"\n",
        "        x = self.dense(hidden_states).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerEndLogits(nn.Module):\n",
        "    \"\"\" Compute SQuAD end_logits from sequence hidden states and start token hidden state.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerEndLogits, self).__init__()\n",
        "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n",
        "        \"\"\" Args:\n",
        "            One of ``start_states``, ``start_positions`` should be not None.\n",
        "            If both are set, ``start_positions`` overrides ``start_states``.\n",
        "\n",
        "            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n",
        "                hidden states of the first tokens for the labeled span.\n",
        "            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "                position of the first token for the labeled span:\n",
        "            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n",
        "                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n",
        "                1.0 means token should be masked.\n",
        "        \"\"\"\n",
        "        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n",
        "        if start_positions is not None:\n",
        "            slen, hsz = hidden_states.shape[-2:]\n",
        "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions) # shape (bsz, 1, hsz)\n",
        "            start_states = start_states.expand(-1, slen, -1) # shape (bsz, slen, hsz)\n",
        "\n",
        "        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n",
        "        x = self.activation(x)\n",
        "        x = self.LayerNorm(x)\n",
        "        x = self.dense_1(x).squeeze(-1)\n",
        "\n",
        "        if p_mask is not None:\n",
        "            x = x * (1 - p_mask) - 1e30 * p_mask\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PoolerAnswerClass(nn.Module):\n",
        "    \"\"\" Compute SQuAD 2.0 answer class from classification and start tokens hidden states. \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(PoolerAnswerClass, self).__init__()\n",
        "        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden_states, start_states=None, start_positions=None, cls_index=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            One of ``start_states``, ``start_positions`` should be not None.\n",
        "            If both are set, ``start_positions`` overrides ``start_states``.\n",
        "\n",
        "            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n",
        "                hidden states of the first tokens for the labeled span.\n",
        "            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "                position of the first token for the labeled span.\n",
        "            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n",
        "                position of the CLS token. If None, take the last token.\n",
        "\n",
        "            note(Original repo):\n",
        "                no dependency on end_feature so that we can obtain one single `cls_logits`\n",
        "                for each sample\n",
        "        \"\"\"\n",
        "        hsz = hidden_states.shape[-1]\n",
        "        assert start_states is not None or start_positions is not None, \"One of start_states, start_positions should be not None\"\n",
        "        if start_positions is not None:\n",
        "            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            start_states = hidden_states.gather(-2, start_positions).squeeze(-2) # shape (bsz, hsz)\n",
        "\n",
        "        if cls_index is not None:\n",
        "            cls_index = cls_index[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n",
        "            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, hsz)\n",
        "        else:\n",
        "            cls_token_state = hidden_states[:, -1, :] # shape (bsz, hsz)\n",
        "\n",
        "        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n",
        "        x = self.activation(x)\n",
        "        x = self.dense_1(x).squeeze(-1)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class SQuADHead(nn.Module):\n",
        "    r\"\"\" A SQuAD head inspired by XLNet.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~pytorch_transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n",
        "\n",
        "    Inputs:\n",
        "        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``\n",
        "            hidden states of sequence tokens\n",
        "        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            position of the first token for the labeled span.\n",
        "        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            position of the last token for the labeled span.\n",
        "        **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n",
        "            position of the CLS token. If None, take the last token.\n",
        "        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``\n",
        "            Whether the question has a possible answer in the paragraph or not.\n",
        "        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n",
        "            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n",
        "            1.0 means token should be masked.\n",
        "\n",
        "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
        "        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
        "            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n",
        "        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n",
        "            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n",
        "        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n",
        "            Indices for the top config.start_n_top start token possibilities (beam-search).\n",
        "        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n",
        "            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
        "        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n",
        "            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n",
        "        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n",
        "            ``torch.FloatTensor`` of shape ``(batch_size,)``\n",
        "            Log probabilities for the ``is_impossible`` label of the answers.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(SQuADHead, self).__init__()\n",
        "        self.start_n_top = config.start_n_top\n",
        "        self.end_n_top = config.end_n_top\n",
        "\n",
        "        self.start_logits = PoolerStartLogits(config)\n",
        "        self.end_logits = PoolerEndLogits(config)\n",
        "        self.answer_class = PoolerAnswerClass(config)\n",
        "\n",
        "    def forward(self, hidden_states, start_positions=None, end_positions=None,\n",
        "                cls_index=None, is_impossible=None, p_mask=None):\n",
        "        outputs = ()\n",
        "\n",
        "        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n",
        "            for x in (start_positions, end_positions, cls_index, is_impossible):\n",
        "                if x is not None and x.dim() > 1:\n",
        "                    x.squeeze_(-1)\n",
        "\n",
        "            # during training, compute the end logits based on the ground truth of the start position\n",
        "            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "            if cls_index is not None and is_impossible is not None:\n",
        "                # Predict answerability from the representation of CLS and START\n",
        "                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n",
        "                loss_fct_cls = nn.BCEWithLogitsLoss()\n",
        "                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n",
        "\n",
        "                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n",
        "                total_loss += cls_loss * 0.5\n",
        "\n",
        "            outputs = (total_loss,) + outputs\n",
        "\n",
        "        else:\n",
        "            # during inference, compute the end logits based on beam search\n",
        "            bsz, slen, hsz = hidden_states.size()\n",
        "            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n",
        "\n",
        "            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n",
        "            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n",
        "            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n",
        "            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n",
        "\n",
        "            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n",
        "            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n",
        "            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n",
        "            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n",
        "\n",
        "            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n",
        "            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n",
        "            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n",
        "\n",
        "            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n",
        "            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n",
        "\n",
        "            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n",
        "\n",
        "        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n",
        "        # or (if labels are provided) (total_loss,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class SequenceSummary(nn.Module):\n",
        "    r\"\"\" Compute a single vector summary of a sequence hidden states according to various possibilities:\n",
        "        Args of the config class:\n",
        "            summary_type:\n",
        "                - 'last' => [default] take the last token hidden state (like XLNet)\n",
        "                - 'first' => take the first token hidden state (like Bert)\n",
        "                - 'mean' => take the mean of all tokens hidden states\n",
        "                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)\n",
        "                - 'attn' => Not implemented now, use multi-head attention\n",
        "            summary_use_proj: Add a projection after the vector extraction\n",
        "            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n",
        "            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default\n",
        "            summary_first_dropout: Add a dropout before the projection and activation\n",
        "            summary_last_dropout: Add a dropout after the projection and activation\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(SequenceSummary, self).__init__()\n",
        "\n",
        "        self.summary_type = config.summary_type if hasattr(config, 'summary_use_proj') else 'last'\n",
        "        if self.summary_type == 'attn':\n",
        "            # We should use a standard multi-head attention module with absolute positional embedding for that.\n",
        "            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n",
        "            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n",
        "            raise NotImplementedError\n",
        "\n",
        "        self.summary = Identity()\n",
        "        if hasattr(config, 'summary_use_proj') and config.summary_use_proj:\n",
        "            if hasattr(config, 'summary_proj_to_labels') and config.summary_proj_to_labels and config.num_labels > 0:\n",
        "                num_classes = config.num_labels\n",
        "            else:\n",
        "                num_classes = config.hidden_size\n",
        "            self.summary = nn.Linear(config.hidden_size, num_classes)\n",
        "\n",
        "        self.activation = Identity()\n",
        "        if hasattr(config, 'summary_activation') and config.summary_activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "\n",
        "        self.first_dropout = Identity()\n",
        "        if hasattr(config, 'summary_first_dropout') and config.summary_first_dropout > 0:\n",
        "            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n",
        "\n",
        "        self.last_dropout = Identity()\n",
        "        if hasattr(config, 'summary_last_dropout') and config.summary_last_dropout > 0:\n",
        "            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n",
        "\n",
        "    def forward(self, hidden_states, cls_index=None):\n",
        "        \"\"\" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n",
        "            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n",
        "                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n",
        "                if summary_type == 'cls_index' and cls_index is None:\n",
        "                    we take the last token of the sequence as classification token\n",
        "        \"\"\"\n",
        "        if self.summary_type == 'last':\n",
        "            output = hidden_states[:, -1]\n",
        "        elif self.summary_type == 'first':\n",
        "            output = hidden_states[:, 0]\n",
        "        elif self.summary_type == 'mean':\n",
        "            output = hidden_states.mean(dim=1)\n",
        "        elif self.summary_type == 'cls_index':\n",
        "            if cls_index is None:\n",
        "                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n",
        "            else:\n",
        "                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n",
        "                cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))\n",
        "            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n",
        "            output = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, XX, hidden_size)\n",
        "        elif self.summary_type == 'attn':\n",
        "            raise NotImplementedError\n",
        "\n",
        "        output = self.first_dropout(output)\n",
        "        output = self.summary(output)\n",
        "        output = self.activation(output)\n",
        "        output = self.last_dropout(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "def prune_linear_layer(layer, index, dim=0):\n",
        "    \"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if layer.bias is not None:\n",
        "        if dim == 1:\n",
        "            b = layer.bias.clone().detach()\n",
        "        else:\n",
        "            b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    if layer.bias is not None:\n",
        "        new_layer.bias.requires_grad = False\n",
        "        new_layer.bias.copy_(b.contiguous())\n",
        "        new_layer.bias.requires_grad = True\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "def prune_conv1d_layer(layer, index, dim=1):\n",
        "    \"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n",
        "        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    index = index.to(layer.weight.device)\n",
        "    W = layer.weight.index_select(dim, index).clone().detach()\n",
        "    if dim == 0:\n",
        "        b = layer.bias.clone().detach()\n",
        "    else:\n",
        "        b = layer.bias[index].clone().detach()\n",
        "    new_size = list(layer.weight.size())\n",
        "    new_size[dim] = len(index)\n",
        "    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n",
        "    new_layer.weight.requires_grad = False\n",
        "    new_layer.weight.copy_(W.contiguous())\n",
        "    new_layer.weight.requires_grad = True\n",
        "    new_layer.bias.requires_grad = False\n",
        "    new_layer.bias.copy_(b.contiguous())\n",
        "    new_layer.bias.requires_grad = True\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "def prune_layer(layer, index, dim=None):\n",
        "    \"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n",
        "        Return the pruned layer as a new layer with requires_grad=True.\n",
        "        Used to remove heads.\n",
        "    \"\"\"\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n",
        "    elif isinstance(layer, Conv1D):\n",
        "        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n",
        "    else:\n",
        "        raise ValueError(\"Can't prune layer of class {}\".format(layer.__class__))\n",
        "\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@author: Dongyu Zhang\n",
        "\"\"\"\n",
        "\n",
        "from pytorch_transformers.modeling_bert import BertEmbeddings, BertEncoder, BertPooler, BertLayerNorm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init, Parameter\n",
        "from torch.nn import CrossEntropyLoss, BCELoss, BCEWithLogitsLoss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class PatientLevelEmbedding(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(PatientLevelEmbedding, self).__init__()\n",
        "        self.config = config\n",
        "        assert self.config.embed_mode in [\"all\", \"note\", \"chunk\", \"no\"]\n",
        "        if self.config.embed_mode == \"all\":\n",
        "            self.note_embedding = nn.Embedding(self.config.max_note_position_embedding, self.config.hidden_size)\n",
        "            self.chunk_embedding = nn.Embedding(self.config.max_chunk_position_embedding, self.config.hidden_size)\n",
        "            self.combine_embed_rep = nn.Linear(self.config.hidden_size * 3, self.config.hidden_size)\n",
        "        elif self.config.embed_mode == \"note\":\n",
        "            self.note_embedding = nn.Embedding(self.config.max_note_position_embedding, self.config.hidden_size)\n",
        "            self.combine_embed_rep = nn.Linear(self.config.hidden_size * 2, self.config.hidden_size)\n",
        "        elif self.config.embed_mode == \"chunk\":\n",
        "            self.chunk_embedding = nn.Embedding(self.config.max_chunk_position_embedding, self.config.hidden_size)\n",
        "            self.combine_embed_rep = nn.Linear(self.config.hidden_size * 2, self.config.hidden_size)\n",
        "        else:\n",
        "            pass\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, inputs, new_note_ids=None, new_chunk_ids=None):\n",
        "        if self.config.embed_mode == \"all\":\n",
        "            note_embeds = self.note_embedding(new_note_ids)\n",
        "            chunk_embeds = self.chunk_embedding(new_chunk_ids)\n",
        "            output = self.combine_embed_rep(torch.cat((inputs, note_embeds, chunk_embeds), 2))\n",
        "        elif self.config.embed_mode == \"note\":\n",
        "            note_embeds = self.note_embedding(new_note_ids)\n",
        "            output = self.combine_embed_rep(torch.cat((inputs, note_embeds), 2))\n",
        "        elif self.config.embed_mode == \"chunk\":\n",
        "            chunk_embeds = self.chunk_embedding(new_chunk_ids)\n",
        "            output = self.combine_embed_rep(torch.cat((inputs, chunk_embeds), 2))\n",
        "        elif self.config.embed_mode == \"no\":\n",
        "            output = inputs\n",
        "        else:\n",
        "            raise ValueError(\"The embed mode: {} is not supported\".format(self.config.embed_mode))\n",
        "        if self.config.embed_mode != \"no\":\n",
        "            output = self.LayerNorm(output)\n",
        "            output = self.dropout(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class SelfDefineBert(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SelfDefineBert, self).__init__()\n",
        "\n",
        "    def init_weights(self, module):\n",
        "        \"\"\" Initialize the weights \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "\n",
        "class PatientLevelBert(SelfDefineBert):\n",
        "    def __init__(self, config):\n",
        "        super(PatientLevelBert, self).__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = PatientLevelEmbedding(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, inputs, new_note_ids=None, new_chunk_ids=None):\n",
        "        device = inputs.device\n",
        "        input_shape = inputs.size()[0:2]\n",
        "        attention_mask = torch.ones(input_shape, device=device)\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        # Provided a padding mask of dimensions [batch_size, seq_length]\n",
        "        # make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if attention_mask.dim() == 3:\n",
        "            extended_attention_mask = attention_mask[:, None, :, :]\n",
        "        if attention_mask.dim() == 2:\n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "        encoder_extended_attention_mask = None\n",
        "        head_mask = [None] * self.config.num_hidden_layers\n",
        "        embedding_output = self.embeddings(inputs=inputs,\n",
        "                                           new_note_ids=new_note_ids,\n",
        "                                           new_chunk_ids=new_chunk_ids)\n",
        "        encoder_outputs = self.encoder(embedding_output,\n",
        "                                       attention_mask=extended_attention_mask,\n",
        "                                       head_mask=head_mask)\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        outputs = (sequence_output, pooled_output,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class PatientLevelBertForSequenceClassification(SelfDefineBert):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(PatientLevelBertForSequenceClassification, self).__init__()\n",
        "        self.config = config\n",
        "        self.patient_bert = PatientLevelBert(config)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, inputs, new_note_ids=None, new_chunk_ids=None, labels=None):\n",
        "        outputs = self.patient_bert(inputs, new_note_ids, new_chunk_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output2 = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output2)\n",
        "        pred = torch.sigmoid(logits).squeeze(1)\n",
        "        if labels is not None:\n",
        "            loss_fct = BCELoss()\n",
        "            loss = loss_fct(pred, labels.float())\n",
        "            return loss, pred\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "\n",
        "class LSTMLayer(SelfDefineBert):\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(LSTMLayer, self).__init__()\n",
        "        self.config = config\n",
        "        self.lstm = nn.LSTM(self.config.hidden_size,\n",
        "                            self.config.hidden_size // 2,\n",
        "                            self.config.lstm_layers,\n",
        "                            batch_first=True,\n",
        "                            bidirectional=True)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.embeddings = PatientLevelEmbedding(config)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, inputs, new_note_ids=None, new_chunk_ids=None, labels=None):\n",
        "        device = inputs.device\n",
        "        batch_size = inputs.size()[0]\n",
        "        hidden = (torch.zeros((self.config.lstm_layers * 2, batch_size, self.config.hidden_size // 2), device=device),\n",
        "                  torch.zeros((self.config.lstm_layers * 2, batch_size, self.config.hidden_size // 2), device=device))\n",
        "        new_input = self.embeddings(inputs, new_note_ids, new_chunk_ids)\n",
        "        lstm_output, hidden = self.lstm(new_input, hidden)\n",
        "        loss_fct = BCELoss()\n",
        "        drop_input = lstm_output[0, -1, :]\n",
        "        class_input = self.dropout(drop_input)\n",
        "        logits = self.classifier(class_input)\n",
        "        pred = torch.sigmoid(logits)\n",
        "        if labels is not None:\n",
        "            loss = loss_fct(pred, labels.float().view(1))\n",
        "            return loss, pred\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "\n",
        "class TLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, config, batch_first=True, bidirectional=True):\n",
        "        super(TLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "        self.c1 = torch.Tensor([1]).float()\n",
        "        self.c2 = torch.Tensor([np.e]).float()\n",
        "        self.ones = torch.ones([1, self.hidden_size]).float()\n",
        "        self.register_buffer('c1_const', self.c1)\n",
        "        self.register_buffer('c2_const', self.c2)\n",
        "        self.register_buffer(\"ones_const\", self.ones)\n",
        "        # Input Gate Parameter\n",
        "        self.Wi = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Ui = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bi = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Forget Gate Parameter\n",
        "        self.Wf = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uf = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bf = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Output Gate Parameter\n",
        "        self.Wog = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uog = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bog = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Cell Layer Parameter\n",
        "        self.Wc = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uc = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bc = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Decomposition Layer Parameter\n",
        "        self.W_decomp = Parameter(\n",
        "            torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.b_decomp = Parameter(torch.zeros(self.hidden_size))\n",
        "\n",
        "    def TLSTM_unit(self, prev_hidden_memory, inputs, times):\n",
        "        prev_hidden_state, prev_cell = prev_hidden_memory\n",
        "        x = inputs\n",
        "        t = times\n",
        "        T = self.map_elapse_time(t)\n",
        "        C_ST = torch.tanh(torch.matmul(prev_cell, self.W_decomp) + self.b_decomp)\n",
        "        C_ST_dis = torch.mul(T, C_ST)\n",
        "        prev_cell = prev_cell - C_ST + C_ST_dis\n",
        "\n",
        "        # Input Gate\n",
        "        i = torch.sigmoid(torch.matmul(x, self.Wi) +\n",
        "                          torch.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
        "        # Forget Gate\n",
        "        f = torch.sigmoid(torch.matmul(x, self.Wf) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
        "        # Output Gate\n",
        "        o = torch.sigmoid(torch.matmul(x, self.Wog) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
        "        # Candidate Memory Cell\n",
        "        C = torch.sigmoid(torch.matmul(x, self.Wc) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
        "        # Current Memory Cell\n",
        "        Ct = f * prev_cell + i * C\n",
        "\n",
        "        # Current Hidden State\n",
        "        current_hidden_state = o * torch.tanh(Ct)\n",
        "\n",
        "        return current_hidden_state, Ct\n",
        "\n",
        "    def map_elapse_time(self, t):\n",
        "        T = torch.div(self.c1_const, torch.log(t + self.c2_const))\n",
        "        T = torch.matmul(T, self.ones_const)\n",
        "        return T\n",
        "\n",
        "    def forward(self, inputs, times):\n",
        "        device = inputs.device\n",
        "        if self.batch_first:\n",
        "            batch_size = inputs.size()[0]\n",
        "            inputs = inputs.permute(1, 0, 2)\n",
        "            times = times.transpose(0, 1)\n",
        "        else:\n",
        "            batch_size = inputs.size()[1]\n",
        "        prev_hidden = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "        prev_cell = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "        seq_len = inputs.size()[0]\n",
        "        hidden_his = []\n",
        "        for i in range(seq_len):\n",
        "            prev_hidden, prev_cell = self.TLSTM_unit((prev_hidden, prev_cell), inputs[i], times[i])\n",
        "            hidden_his.append(prev_hidden)\n",
        "        hidden_his = torch.stack(hidden_his)\n",
        "        if self.bidirectional:\n",
        "            second_hidden = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "            second_cell = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "            second_inputs = torch.flip(inputs, [0])\n",
        "            second_times = torch.flip(times, [0])\n",
        "            second_hidden_his = []\n",
        "            for i in range(seq_len):\n",
        "                if i == 0:\n",
        "                    time = times[i]\n",
        "                else:\n",
        "                    time = second_times[i-1]\n",
        "                second_hidden, second_cell = self.TLSTM_unit((second_hidden, second_cell), second_inputs[i], time)\n",
        "                second_hidden_his.append(second_hidden)\n",
        "            second_hidden_his = torch.stack(second_hidden_his)\n",
        "            hidden_his = torch.cat((hidden_his, second_hidden_his), dim=2)\n",
        "            prev_hidden = torch.cat((prev_hidden, second_hidden), dim=1)\n",
        "            prev_cell = torch.cat((prev_cell, second_cell), dim=1)\n",
        "        if self.batch_first:\n",
        "            hidden_his = hidden_his.permute(1, 0, 2)\n",
        "        return hidden_his, (prev_hidden, prev_cell)\n",
        "\n",
        "\n",
        "class TLSTMLayer(SelfDefineBert):\n",
        "\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(TLSTMLayer, self).__init__()\n",
        "        self.config = config\n",
        "        self.tlstm = TLSTM(self.config.hidden_size,\n",
        "                           self.config.hidden_size // 2,\n",
        "                           self.config,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=True)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.embeddings = PatientLevelEmbedding(config)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, inputs, times, new_note_ids=None, new_chunk_ids=None, labels=None):\n",
        "        new_input = self.embeddings(inputs, new_note_ids, new_chunk_ids)\n",
        "        lstm_output, hidden = self.tlstm(new_input, times.float())\n",
        "        loss_fct = BCEWithLogitsLoss()\n",
        "        drop_input = lstm_output[0, -1, :]\n",
        "        class_input = self.dropout(drop_input)\n",
        "        logits = self.classifier(class_input)\n",
        "        logits = torch.where(torch.isnan(logits), torch.zeros_like(logits), logits)\n",
        "        logits = torch.where(torch.isinf(logits), torch.zeros_like(logits), logits)\n",
        "        pred = torch.sigmoid(logits)\n",
        "        pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred)\n",
        "        pred = torch.where(torch.isinf(pred), torch.zeros_like(pred), pred)\n",
        "        if labels is not None:\n",
        "            loss = loss_fct(logits, labels.float().view(1))\n",
        "            return loss, pred\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "\n",
        "class FTLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, config, batch_first=True, bidirectional=True):\n",
        "        super(FTLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.batch_first = batch_first\n",
        "        self.bidirectional = bidirectional\n",
        "        self.c1 = torch.Tensor([1]).float()\n",
        "        self.c2 = torch.Tensor([np.e]).float()\n",
        "        self.c3 = torch.Tensor([0.]).float()\n",
        "        self.ones = torch.ones([1, self.hidden_size]).float()\n",
        "        self.register_buffer('c1_const', self.c1)\n",
        "        self.register_buffer('c2_const', self.c2)\n",
        "        self.register_buffer('c3_const', self.c3)\n",
        "        self.register_buffer(\"ones_const\", self.ones)\n",
        "        # Input Gate Parameter\n",
        "        self.Wi = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Ui = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bi = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Forget Gate Parameter\n",
        "        self.Wf = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uf = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bf = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Output Gate Parameter\n",
        "        self.Wog = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uog = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bog = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Cell Layer Parameter\n",
        "        self.Wc = Parameter(torch.normal(0.0, config.initializer_range, size=(self.input_size, self.hidden_size)))\n",
        "        self.Uc = Parameter(torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.bc = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Decomposition Layer Parameter\n",
        "        self.W_decomp = Parameter(\n",
        "            torch.normal(0.0, config.initializer_range, size=(self.hidden_size, self.hidden_size)))\n",
        "        self.b_decomp = Parameter(torch.zeros(self.hidden_size))\n",
        "        # Decay Parameter\n",
        "        self.W_decay_1 = Parameter(torch.tensor([[0.33]]))\n",
        "        self.W_decay_2 = Parameter(torch.tensor([[0.33]]))\n",
        "        self.W_decay_3 = Parameter(torch.tensor([[0.33]]))\n",
        "        self.a = Parameter(torch.tensor([1.0]))\n",
        "        self.b = Parameter(torch.tensor([1.0]))\n",
        "        self.m = Parameter(torch.tensor([0.02]))\n",
        "        self.k = Parameter(torch.tensor([2.9]))\n",
        "        self.d = Parameter(torch.tensor([4.5]))\n",
        "        self.n = Parameter(torch.tensor([2.5]))\n",
        "\n",
        "    def FTLSTM_unit(self, prev_hidden_memory, inputs, times):\n",
        "        prev_hidden_state, prev_cell = prev_hidden_memory\n",
        "        x = inputs\n",
        "        t = times\n",
        "        T = self.map_elapse_time(t)\n",
        "        C_ST = torch.tanh(torch.matmul(prev_cell, self.W_decomp) + self.b_decomp)\n",
        "        C_ST_dis = torch.mul(T, C_ST)\n",
        "        prev_cell = prev_cell - C_ST + C_ST_dis\n",
        "\n",
        "        # Input Gate\n",
        "        i = torch.sigmoid(torch.matmul(x, self.Wi) +\n",
        "                          torch.matmul(prev_hidden_state, self.Ui) + self.bi)\n",
        "        # Forget Gate\n",
        "        f = torch.sigmoid(torch.matmul(x, self.Wf) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uf) + self.bf)\n",
        "        # Output Gate\n",
        "        o = torch.sigmoid(torch.matmul(x, self.Wog) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uog) + self.bog)\n",
        "        # Candidate Memory Cell\n",
        "        C = torch.sigmoid(torch.matmul(x, self.Wc) +\n",
        "                          torch.matmul(prev_hidden_state, self.Uc) + self.bc)\n",
        "        # Current Memory Cell\n",
        "        Ct = f * prev_cell + i * C\n",
        "\n",
        "        # Current Hidden State\n",
        "        current_hidden_state = o * torch.tanh(Ct)\n",
        "\n",
        "        return current_hidden_state, Ct\n",
        "\n",
        "    def map_elapse_time(self, t):\n",
        "        T_1 = torch.div(self.c1_const, torch.mul(self.a, torch.pow(t, self.b)))\n",
        "        T_2 = self.k - torch.mul(self.m, t)\n",
        "        T_3 = torch.div(self.c1_const, (self.c1_const + torch.pow(torch.div(t, self.d), self.n)))\n",
        "        T = torch.mul(self.W_decay_1, T_1) + torch.mul(self.W_decay_2, T_2) + torch.mul(self.W_decay_3, T_3)\n",
        "        T = torch.max(T, self.c3_const)\n",
        "        T = torch.min(T, self.c1_const)\n",
        "        T = torch.matmul(T, self.ones_const)\n",
        "        return T\n",
        "\n",
        "    def forward(self, inputs, times):\n",
        "        device = inputs.device\n",
        "        if self.batch_first:\n",
        "            batch_size = inputs.size()[0]\n",
        "            inputs = inputs.permute(1, 0, 2)\n",
        "            times = times.transpose(0, 1)\n",
        "        else:\n",
        "            batch_size = inputs.size()[1]\n",
        "        prev_hidden = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "        prev_cell = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "        seq_len = inputs.size()[0]\n",
        "        hidden_his = []\n",
        "        for i in range(seq_len):\n",
        "            prev_hidden, prev_cell = self.FTLSTM_unit((prev_hidden, prev_cell), inputs[i], times[i])\n",
        "            hidden_his.append(prev_hidden)\n",
        "        hidden_his = torch.stack(hidden_his)\n",
        "        if self.bidirectional:\n",
        "            second_hidden = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "            second_cell = torch.zeros((batch_size, self.hidden_size), device=device)\n",
        "            second_inputs = torch.flip(inputs, [0])\n",
        "            second_times = torch.flip(times, [0])\n",
        "            second_hidden_his = []\n",
        "            for i in range(seq_len):\n",
        "                if i == 0:\n",
        "                    time = times[i]\n",
        "                else:\n",
        "                    time = second_times[i-1]\n",
        "                second_hidden, second_cell = self.FTLSTM_unit((second_hidden, second_cell), second_inputs[i], time)\n",
        "                second_hidden_his.append(second_hidden)\n",
        "            second_hidden_his = torch.stack(second_hidden_his)\n",
        "            hidden_his = torch.cat((hidden_his, second_hidden_his), dim=2)\n",
        "            prev_hidden = torch.cat((prev_hidden, second_hidden), dim=1)\n",
        "            prev_cell = torch.cat((prev_cell, second_cell), dim=1)\n",
        "        if self.batch_first:\n",
        "            hidden_his = hidden_his.permute(1, 0, 2)\n",
        "        return hidden_his, (prev_hidden, prev_cell)\n",
        "\n",
        "\n",
        "class FTLSTMLayer(SelfDefineBert):\n",
        "\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(FTLSTMLayer, self).__init__()\n",
        "        self.config = config\n",
        "        self.ftlstm = FTLSTM(self.config.hidden_size,\n",
        "                           self.config.hidden_size // 2,\n",
        "                           self.config,\n",
        "                           batch_first=True,\n",
        "                           bidirectional=True)\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.embeddings = PatientLevelEmbedding(config)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, num_labels)\n",
        "\n",
        "        self.apply(self.init_weights)\n",
        "\n",
        "    def forward(self, inputs, times, new_note_ids=None, new_chunk_ids=None, labels=None):\n",
        "        new_input = self.embeddings(inputs, new_note_ids, new_chunk_ids)\n",
        "        lstm_output, hidden = self.ftlstm(new_input, times.float())\n",
        "        loss_fct = BCEWithLogitsLoss()\n",
        "        drop_input = lstm_output[0, -1, :]\n",
        "        class_input = self.dropout(drop_input)\n",
        "        logits = self.classifier(class_input)\n",
        "        logits = torch.where(torch.isnan(logits), torch.zeros_like(logits), logits)\n",
        "        logits = torch.where(torch.isinf(logits), torch.zeros_like(logits), logits)\n",
        "        pred = torch.sigmoid(logits)\n",
        "        pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred)\n",
        "        pred = torch.where(torch.isinf(pred), torch.zeros_like(pred), pred)\n",
        "        if labels is not None:\n",
        "            loss = loss_fct(logits, labels.float().view(1))\n",
        "            return loss, pred\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch BERT model.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import logging\n",
        "import tarfile\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss, BCELoss\n",
        "\n",
        "\n",
        "print('in the modeling class')\n",
        "\n",
        "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
        "                    level = logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-base-multilingual': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "CONFIG_NAME = 'bert_config.json'\n",
        "WEIGHTS_NAME = 'pytorch_model.bin'\n",
        "\n",
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class BertConfig(object):\n",
        "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocab_size_or_config_json_file,\n",
        "                 hidden_size=768,\n",
        "                 num_hidden_layers=12,\n",
        "                 num_attention_heads=12,\n",
        "                 intermediate_size=3072,\n",
        "                 hidden_act=\"gelu\",\n",
        "                 hidden_dropout_prob=0.1,\n",
        "                 attention_probs_dropout_prob=0.1,\n",
        "                 max_position_embeddings=512,\n",
        "                 type_vocab_size=2,\n",
        "                 initializer_range=0.02):\n",
        "        \"\"\"Constructs BertConfig.\n",
        "\n",
        "        Args:\n",
        "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
        "            hidden_size: Size of the encoder layers and the pooler layer.\n",
        "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
        "            num_attention_heads: Number of attention heads for each attention layer in\n",
        "                the Transformer encoder.\n",
        "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
        "                layer in the Transformer encoder.\n",
        "            hidden_act: The non-linear activation function (function or string) in the\n",
        "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
        "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
        "                layers in the embeddings, encoder, and pooler.\n",
        "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
        "                probabilities.\n",
        "            max_position_embeddings: The maximum sequence length that this model might\n",
        "                ever be used with. Typically set this to something large just in case\n",
        "                (e.g., 512 or 1024 or 2048).\n",
        "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
        "                `BertModel`.\n",
        "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
        "                initializing all weight matrices.\n",
        "        \"\"\"\n",
        "        if isinstance(vocab_size_or_config_json_file, str):\n",
        "            with open(vocab_size_or_config_json_file, \"r\") as reader:\n",
        "                json_config = json.loads(reader.read())\n",
        "            for key, value in json_config.items():\n",
        "                self.__dict__[key] = value\n",
        "        elif isinstance(vocab_size_or_config_json_file, int):\n",
        "            self.vocab_size = vocab_size_or_config_json_file\n",
        "            self.hidden_size = hidden_size\n",
        "            self.num_hidden_layers = num_hidden_layers\n",
        "            self.num_attention_heads = num_attention_heads\n",
        "            self.hidden_act = hidden_act\n",
        "            self.intermediate_size = intermediate_size\n",
        "            self.hidden_dropout_prob = hidden_dropout_prob\n",
        "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "            self.max_position_embeddings = max_position_embeddings\n",
        "            self.type_vocab_size = type_vocab_size\n",
        "            self.initializer_range = initializer_range\n",
        "        else:\n",
        "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
        "                             \"or the path to a pretrained model config file (str)\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, json_object):\n",
        "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
        "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
        "        for key, value in json_object.items():\n",
        "            config.__dict__[key] = value\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_json_file(cls, json_file):\n",
        "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
        "        with open(json_file, \"r\") as reader:\n",
        "            text = reader.read()\n",
        "        return cls.from_dict(json.loads(text))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class BertLayerNorm(nn.Module):\n",
        "    def __init__(self, config, variance_epsilon=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BertLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(config.hidden_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(config.hidden_size))\n",
        "        self.variance_epsilon = variance_epsilon\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.transform_act_fn = ACT2FN[config.hidden_act] \\\n",
        "            if isinstance(config.hidden_act, str) else config.hidden_act\n",
        "        self.LayerNorm = BertLayerNorm(config)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLMPredictionHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertLMPredictionHead, self).__init__()\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n",
        "                                 bert_model_embedding_weights.size(0),\n",
        "                                 bias=False)\n",
        "        self.decoder.weight = bert_model_embedding_weights\n",
        "        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertOnlyMLMHead(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertOnlyMLMHead, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "\n",
        "    def forward(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores\n",
        "\n",
        "\n",
        "class BertOnlyNSPHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOnlyNSPHead, self).__init__()\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class PreTrainedBertModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(PreTrainedBertModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.beta.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            module.gamma.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a PreTrainedBertModel from a pre-trained model file.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "\n",
        "        Params:\n",
        "            pretrained_model_name: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-base-multilingual`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file)\n",
        "        except FileNotFoundError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    pretrained_model_name))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file):\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "        state_dict = torch.load(weights_path, map_location = 'cpu')\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        load(model, prefix='' if hasattr(model, 'bert') else 'bert.')\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        return model\n",
        "\n",
        "\n",
        "class BertModel(PreTrainedBertModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block,\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLF`) to train on the Next-Sentence task (see BERT's paper).\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = modeling.BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output\n",
        "\n",
        "\n",
        "class BertForPreTraining(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with pre-training heads.\n",
        "    This module comprises the BERT model followed by the two pre-training heads:\n",
        "        - the masked language modeling head, and\n",
        "        - the next sentence classification head.\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `masked_lm_labels` or `next_sentence_label` is `None`:\n",
        "            Outputs a tuple comprising\n",
        "            - the masked language modeling logits, and\n",
        "            - the next sentence classification logits.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForPreTraining(config)\n",
        "    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForPreTraining, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertPreTrainingHeads(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n",
        "        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                                   output_all_encoded_layers=False)\n",
        "        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n",
        "\n",
        "        if masked_lm_labels is not None and next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores, masked_lm_labels)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score, next_sentence_label)\n",
        "            total_loss = masked_lm_loss + next_sentence_loss\n",
        "            return total_loss\n",
        "        else:\n",
        "            return prediction_scores, seq_relationship_score\n",
        "\n",
        "\n",
        "class BertForMaskedLM(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with the masked language modeling head.\n",
        "    This module comprises the BERT model followed by the masked language modeling head.\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n",
        "            is only computed for the labels set in [0, ..., vocab_size]\n",
        "\n",
        "    Outputs:\n",
        "        if `masked_lm_labels` is `None`:\n",
        "            Outputs the masked language modeling loss.\n",
        "        if `masked_lm_labels` is `None`:\n",
        "            Outputs the masked language modeling logits.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForMaskedLM(config)\n",
        "    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForMaskedLM, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                       output_all_encoded_layers=False)\n",
        "        prediction_scores = self.cls(sequence_output)\n",
        "\n",
        "        if masked_lm_labels is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            masked_lm_loss = loss_fct(prediction_scores, masked_lm_labels)\n",
        "            return masked_lm_loss\n",
        "        else:\n",
        "            return prediction_scores\n",
        "\n",
        "\n",
        "class BertForNextSentencePrediction(PreTrainedBertModel):\n",
        "    \"\"\"BERT model with next sentence prediction head.\n",
        "    This module comprises the BERT model followed by the next sentence classification head.\n",
        "\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, 1].\n",
        "            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n",
        "\n",
        "    Outputs:\n",
        "        if `next_sentence_label` is not `None`:\n",
        "            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n",
        "            sentence classification loss.\n",
        "        if `next_sentence_label` is `None`:\n",
        "            Outputs the next sentence classification logits.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForNextSentencePrediction(config)\n",
        "    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForNextSentencePrediction, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.cls = BertOnlyNSPHead(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, next_sentence_label=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n",
        "                                     output_all_encoded_layers=False)\n",
        "        seq_relationship_score = self.cls( pooled_output)\n",
        "\n",
        "        if next_sentence_label is not None:\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=-1)\n",
        "            next_sentence_loss = loss_fct(seq_relationship_score, next_sentence_label)\n",
        "            return next_sentence_loss\n",
        "        else:\n",
        "            return seq_relationship_score\n",
        "\n",
        "\n",
        "class BertForSequenceClassification(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for classification.\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the pooled output.\n",
        "\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
        "            with indices selected in [0, ..., num_labels].\n",
        "\n",
        "    Outputs:\n",
        "        if `labels` is not `None`:\n",
        "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
        "        if `labels` is `None`:\n",
        "            Outputs the classification logits.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    num_labels = 2\n",
        "\n",
        "    model = BertForSequenceClassification(config, num_labels)\n",
        "    logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config, num_labels):\n",
        "        super(BertForSequenceClassification, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "\n",
        "        pooled_output2 = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output2)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = BCELoss()\n",
        "            m = nn.Sigmoid()\n",
        "            n = torch.squeeze(m(logits))\n",
        "            loss = loss_fct(n, labels.float())\n",
        "            return loss, logits\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "class BertForQuestionAnswering(PreTrainedBertModel):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "\n",
        "    Params:\n",
        "        `config`: either\n",
        "            - a BertConfig class instance with the configuration to build a new model, or\n",
        "            - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                . `bert-base-uncased`\n",
        "                . `bert-large-uncased`\n",
        "                . `bert-base-cased`\n",
        "                . `bert-base-multilingual`\n",
        "                . `bert-base-chinese`\n",
        "                The pre-trained model will be downloaded and cached if needed.\n",
        "\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "\n",
        "    Outputs:\n",
        "        if `start_positions` and `end_positions` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
        "        if `start_positions` or `end_positions` is `None`:\n",
        "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
        "            position tokens.\n",
        "\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 2, 0]])\n",
        "\n",
        "    config = BertConfig(vocab_size=32000, hidden_size=512,\n",
        "        num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)\n",
        "\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suU9lzKjy6pW",
        "outputId": "7f088b05-610e-4ac2-8262-b2e35bde3220"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the modeling class\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enGK6MY42rR4",
        "outputId": "8c85b961-b632-41b9-8d2d-7649e18736fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotmap import DotMap\n",
        "from pytorch_transformers import BertTokenizer\n",
        "import six\n",
        "from sklearn.metrics import roc_curve, precision_recall_curve, \\\n",
        "    auc, matthews_corrcoef, accuracy_score, precision_score, recall_score, f1_score\n",
        "import time\n",
        "\n",
        "def concat_by_id_list_with_note_chunk_id_time(df, labels, inputs, masks, note_ids, times, str_len):\n",
        "    final_labels, final_inputs, final_masks, final_note_ids, final_chunk_ids, final_times = [], [], [], [], [], []\n",
        "    id_lists = df.Adm_ID.unique()\n",
        "    for id in id_lists:\n",
        "        id_ix = df.index[df.Adm_ID == id].to_list()\n",
        "        final_inputs.append(inputs[id_ix])\n",
        "        final_masks.append(masks[id_ix])\n",
        "        final_labels.append(labels[id_ix].max())\n",
        "        final_note_ids.append(note_ids[id_ix])\n",
        "        final_chunk_ids.append(torch.tensor(list(range(len(id_ix)))[::-1]))\n",
        "        final_times.append(torch.tensor(np.concatenate([np.zeros(1), np.diff(times[id_ix])])))\n",
        "    return final_labels, final_inputs, final_masks, id_lists, final_note_ids, final_chunk_ids, final_times\n",
        "\n",
        "\n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
        "                  padding='pre', truncating='pre', value=0.):\n",
        "    \"\"\"Pads sequences to the same length.\n",
        "\n",
        "    This function transforms a list of\n",
        "    `num_samples` sequences (lists of integers)\n",
        "    into a 2D Numpy array of shape `(num_samples, num_timesteps)`.\n",
        "    `num_timesteps` is either the `maxlen` argument if provided,\n",
        "    or the length of the longest sequence otherwise.\n",
        "\n",
        "    Sequences that are shorter than `num_timesteps`\n",
        "    are padded with `value` at the end.\n",
        "\n",
        "    Sequences longer than `num_timesteps` are truncated\n",
        "    so that they fit the desired length.\n",
        "    The position where padding or truncation happens is determined by\n",
        "    the arguments `padding` and `truncating`, respectively.\n",
        "\n",
        "    Pre-padding is the default.\n",
        "\n",
        "    # Arguments\n",
        "        sequences: List of lists, where each element is a sequence.\n",
        "        maxlen: Int, maximum length of all sequences.\n",
        "        dtype: Type of the output sequences.\n",
        "            To pad sequences with variable length strings, you can use `object`.\n",
        "        padding: String, 'pre' or 'post':\n",
        "            pad either before or after each sequence.\n",
        "        truncating: String, 'pre' or 'post':\n",
        "            remove values from sequences larger than\n",
        "            `maxlen`, either at the beginning or at the end of the sequences.\n",
        "        value: Float or String, padding value.\n",
        "\n",
        "    # Returns\n",
        "        x: Numpy array with shape `(len(sequences), maxlen)`\n",
        "\n",
        "    # Raises\n",
        "        ValueError: In case of invalid values for `truncating` or `padding`,\n",
        "            or in case of invalid shape for a `sequences` entry.\n",
        "    \"\"\"\n",
        "    if not hasattr(sequences, '__len__'):\n",
        "        raise ValueError('`sequences` must be iterable.')\n",
        "    num_samples = len(sequences)\n",
        "\n",
        "    lengths = []\n",
        "    for x in sequences:\n",
        "        try:\n",
        "            lengths.append(len(x))\n",
        "        except TypeError:\n",
        "            raise ValueError('`sequences` must be a list of iterables. '\n",
        "                             'Found non-iterable: ' + str(x))\n",
        "\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n",
        "    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n",
        "        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n",
        "                         \"You should set `dtype=object` for variable length strings.\"\n",
        "                         .format(dtype, type(value)))\n",
        "\n",
        "    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if not len(s):\n",
        "            continue  # empty list/array was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" '\n",
        "                             'not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s '\n",
        "                             'is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x\n",
        "\n",
        "def Tokenize_with_note_id_hour(df, max_length, tokenizer):\n",
        "    labels = df.Label.values\n",
        "    note_ids = df.Note_ID.values\n",
        "    times = pd.to_datetime(df.charttime.values)\n",
        "    times = times - times.min()\n",
        "    times = times / pd.Timedelta(days=1)\n",
        "\n",
        "    assert 'Input_ID' in df.columns\n",
        "    input_ids = df.Input_ID.apply(lambda x: x[1:-1].split(', '))\n",
        "    input_ids = input_ids.apply(lambda x: [int(i) for i in x])\n",
        "    input_ids = input_ids.values\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    attention_masks = []\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i > 0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "    return labels, input_ids, attention_masks, note_ids, times\n",
        "\n",
        "def convert_note_ids(note_ids):\n",
        "    new_dict = dict(zip(pd.Series(note_ids).unique(), range(len(pd.Series(note_ids).unique()))[::-1]))\n",
        "    new_ids = [new_dict[i] for i in note_ids]\n",
        "    return torch.tensor(new_ids)\n",
        "\n",
        "def get_patient_score(df, c):\n",
        "    df_sort = df.sort_values(by=['Adm_ID'])\n",
        "    # score\n",
        "    temp = (df_sort.groupby(['Adm_ID'])['logits'].agg(max) + df_sort.groupby(['Adm_ID'])['logits'].agg(\n",
        "        sum) / c) / (1 + df_sort.groupby(['Adm_ID'])['logits'].agg(len) / c)\n",
        "    x = df_sort.groupby(['Adm_ID'])['label'].agg(np.min).values\n",
        "    predictions = (temp.values >= 0.5).astype(np.int)\n",
        "    ids = df_sort['Adm_ID'].unique()\n",
        "    df_out = pd.DataFrame({'logits': temp.values, 'pred_label': predictions, 'label': x, 'Adm_ID': ids})\n",
        "    return df_out\n",
        "\n",
        "\n",
        "def test_func(sublist):\n",
        "    if sublist.shape is ():\n",
        "        return [sublist.tolist()]\n",
        "    else:\n",
        "        return sublist\n",
        "\n",
        "def concat_by_id_list_with_note_chunk_id(df, labels, inputs, masks, note_ids, str_len):\n",
        "    final_labels, final_inputs, final_masks, final_note_ids, final_chunk_ids = [], [], [], [], []\n",
        "    id_lists = df.Adm_ID.unique()\n",
        "    for id in id_lists:\n",
        "        id_ix = df.index[df.Adm_ID == id].to_list()\n",
        "        final_inputs.append(inputs[id_ix])\n",
        "        final_masks.append(masks[id_ix])\n",
        "        final_labels.append(labels[id_ix].max())\n",
        "        final_note_ids.append(note_ids[id_ix])\n",
        "        final_chunk_ids.append(torch.tensor(list(range(len(id_ix)))[::-1]))\n",
        "    return final_labels, final_inputs, final_masks, id_lists, final_note_ids, final_chunk_ids\n",
        "\n",
        "def reorder_by_time(data):\n",
        "    data.chartdate = pd.to_datetime(data.chartdate)\n",
        "    data.charttime = pd.to_datetime(data.charttime)\n",
        "    data.loc[data.charttime.isna(), 'charttime'] = data[data.charttime.isna()].chartdate + pd.Timedelta(hours=23,\n",
        "                                                                                                        minutes=59,\n",
        "                                                                                                        seconds=59)\n",
        "    data = data.sort_values(by=['Adm_ID', 'charttime', 'Note_ID'])\n",
        "    data.reset_index(inplace=True)\n",
        "    return data\n",
        "\n",
        "\n",
        "def model_auc(y_true, y_pred):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "    return auc_score, fpr, tpr, thresholds\n",
        "\n",
        "\n",
        "def model_aupr(y_true, y_pred):\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
        "    aupr_score = auc(recall, precision)\n",
        "    return aupr_score, precision, recall, thresholds\n",
        "\n",
        "\n",
        "def write_performance(flat_true_labels, flat_predictions, flat_logits, config):\n",
        "    test_accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "\n",
        "    test_f1 = f1_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_prec = precision_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_rec = recall_score(flat_true_labels, flat_predictions, average='binary')\n",
        "\n",
        "    test_auc, _, _, _ = model_auc(flat_true_labels, flat_logits)\n",
        "\n",
        "    test_mc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "    test_aupr, _, _, _ = model_aupr(flat_true_labels, flat_logits)\n",
        "\n",
        "    test_msl = 64\n",
        "\n",
        "    test_seed = 42\n",
        "\n",
        "    test_dir_code = \"./Ecoli\"\n",
        "\n",
        "    test_time = time.ctime()\n",
        "\n",
        "    exp_path = \"{}_{}_{}.csv\".format(config.task_name, config.embed_mode, test_msl)\n",
        "\n",
        "    header = \"Len,Dir,Seed,Accuracy,F1_Score,Precision,Recall,AUC,MCC,AUPR,Time\"\n",
        "    content = \"{},{},{},{},{},{},{},{},{},{},{}\".format(test_msl,\n",
        "                                                        test_dir_code,\n",
        "                                                        test_seed,\n",
        "                                                        test_accuracy,\n",
        "                                                        test_f1,\n",
        "                                                        test_prec,\n",
        "                                                        test_rec,\n",
        "                                                        test_auc,\n",
        "                                                        test_mc,\n",
        "                                                        test_aupr,\n",
        "                                                        test_time)\n",
        "\n",
        "    print(\"Test Patient Level Accuracy: {}\\n\"\n",
        "              \"Test Patient Level F1 Score: {}\\n\"\n",
        "              \"Test Patient Level Precision: {}\\n\"\n",
        "              \"Test Patient Level Recall: {}\\n\"\n",
        "              \"Test Patient Level AUC: {} \\n\"\n",
        "              \"Test Patient Level Matthew's correlation coefficient: {}\\n\"\n",
        "              \"Test Patient Level AUPR: {} \\n\"\n",
        "              \"All Finished!\".format(test_accuracy,\n",
        "                                     test_f1,\n",
        "                                     test_prec,\n",
        "                                     test_rec,\n",
        "                                     test_auc,\n",
        "                                     test_mc,\n",
        "                                     test_aupr))\n",
        "\n",
        "\n",
        "MAX_LEN = 64\n",
        "max_chunk_num = 32\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
      ],
      "metadata": {
        "id": "gBdVZoTvsSFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66368d9f-3999-43e4-c49b-731f3b1d9e33"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:153: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:153: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-9-e4f1ca793a61>:153: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if sublist.shape is ():\n",
            "100%|██████████| 231508/231508 [00:00<00:00, 4427469.63B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZY-caGLcT9O",
        "outputId": "6ed9ab56-904f-4dfd-940f-3dd34d376f11"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.25.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.34.100)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.12.25)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert) (12.4.127)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.100 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.34.100)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.100->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.100->boto3->pytorch_pretrained_bert) (1.16.0)\n",
            "Installing collected packages: pytorch_pretrained_bert\n",
            "Successfully installed pytorch_pretrained_bert-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "max_seq_length: 64,  max_chunk_num: 32,\n",
        "train_batch_size: 1, eval_batch_size: 1,\n",
        "learning_rate: 1e-07, warmup_proportion: 0.1,\n",
        "num_train_epochs: 3, seed: 42, gradient_accumulation_steps: 1,\n",
        "hidden_dropout_prob: 0.1 ,\n",
        "layer_norm_eps: 1e-12,\n",
        "initializer_range: 0.02,\n",
        "max_note_position_embedding: 1000,\n",
        "max_chunk_position_embedding: 1000,\n",
        "hidden_size: 768\n",
        "\n",
        "Number of GPU is 1,\n",
        "Device Name: NVIDIA GeForce RTX 4060 Laptop GPU\n",
        "\n",
        "total training time is: 1728.1761891841888s for CIBERT-am (no embedding),\n",
        "1163.5464289188385s s for CIBERT-am (all embedding), 1299.2628262043s for TL-Trans (all embedding)\n",
        "\n",
        "\n",
        "Evaluation metrics:\n",
        "\n",
        "Accuracy: The proportion of true results (both true positives and true negatives) among the total number of cases examined.\n",
        "\n",
        "F1 Score: The harmonic mean of precision and recall.\n",
        "\n",
        "Precision: The ratio of correctly predicted positive observations to the total predicted positives.\n",
        "\n",
        "Recall (Sensitivity): The ratio of correctly predicted positive observations to all observations in actual class.\n",
        "\n",
        "Area Under the Receiver Operating Characteristic Curve (AUROC): AUROC represents the likelihood of the model distinguishing between the classes.\n",
        "\n",
        "Matthew's Correlation Coefficient (MCC):  A coefficient that produces a high score only if the prediction obtained good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both within predicted classes and actual classes.\n",
        "\n",
        "Area Under the Precision-Recall Curve (AUPR): AUPR measures the relationship between precision and recall for different thresholds.\n",
        "\n"
      ],
      "metadata": {
        "id": "CGeK4Pe2VyeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #!/usr/bin/env python3\n",
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"\n",
        "# @author: Dongyu Zhang\n",
        "# \"\"\"\n",
        "\n",
        "# import argparse\n",
        "# import os\n",
        "# import random\n",
        "# import time\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()\n",
        "# from dotmap import DotMap\n",
        "# from pytorch_pretrained_bert.optimization import BertAdam\n",
        "# from pytorch_transformers import BertForSequenceClassification\n",
        "# from pytorch_transformers import BertTokenizer\n",
        "# from tqdm import trange\n",
        "# from other_func import write_log, Tokenize_with_note_id, concat_by_id_list_with_note_chunk_id, flat_accuracy, \\\n",
        "#     write_performance\n",
        "# from utils import mask_batch_generator\n",
        "\n",
        "\n",
        "# def get_patient_score(df, c):\n",
        "#     df_sort = df.sort_values(by=['Adm_ID'])\n",
        "#     # score\n",
        "#     temp = (df_sort.groupby(['Adm_ID'])['logits'].agg(max) + df_sort.groupby(['Adm_ID'])['logits'].agg(\n",
        "#         sum) / c) / (1 + df_sort.groupby(['Adm_ID'])['logits'].agg(len) / c)\n",
        "#     x = df_sort.groupby(['Adm_ID'])['label'].agg(np.min).values\n",
        "#     predictions = (temp.values >= 0.5).astype(np.int)\n",
        "#     ids = df_sort['Adm_ID'].unique()\n",
        "#     df_out = pd.DataFrame({'logits': temp.values, 'pred_label': predictions, 'label': x, 'Adm_ID': ids})\n",
        "#     return df_out\n",
        "\n",
        "\n",
        "# def test_func(sublist):\n",
        "#     if sublist.shape is ():\n",
        "#         return [sublist.tolist()]\n",
        "#     else:\n",
        "#         return sublist\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     ## Required parameters\n",
        "#     parser.add_argument(\"--data_dir\",\n",
        "#                         default='./Ecoli',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--train_data\",\n",
        "#                         default='train.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input training data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--val_data\",\n",
        "#                         default='val.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input validation data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--test_data\",\n",
        "#                         default='test.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input test data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--log_path\",\n",
        "#                         default='./log.txt',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The log file path.\")\n",
        "\n",
        "#     parser.add_argument(\"--output_dir\",\n",
        "#                         default='./exp_FTL-Trans',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "#     parser.add_argument(\"--save_model\",\n",
        "#                         default=True,\n",
        "#                         action='store_true',\n",
        "#                         help=\"Whether to save the model.\")\n",
        "\n",
        "#     parser.add_argument(\"--bert_model\",\n",
        "#                         default=\"bert-base-uncased\",\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
        "\n",
        "#     parser.add_argument(\"--embed_mode\",\n",
        "#                         default='all',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The embedding type selected in the list: all, note, chunk, no.\")\n",
        "\n",
        "#     parser.add_argument(\"--c\",\n",
        "#                         default=0.1,\n",
        "#                         type=float,\n",
        "#                         required=False,\n",
        "#                         help=\"The parameter c for scaled adjusted mean method\")\n",
        "\n",
        "#     parser.add_argument(\"--task_name\",\n",
        "#                         default=\"BERT_ecoli_am\",\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The name of the task.\")\n",
        "\n",
        "#     ## Other parameters\n",
        "#     parser.add_argument(\"--max_seq_length\",\n",
        "#                         default=64,\n",
        "#                         type=int,\n",
        "#                         help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "#                              \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "#                              \"than this will be padded.\")\n",
        "#     parser.add_argument(\"--max_chunk_num\",\n",
        "#                         default=32,\n",
        "#                         type=int,\n",
        "#                         help=\"The maximum total input chunk numbers after WordPiece tokenization.\")\n",
        "#     parser.add_argument(\"--train_batch_size\",\n",
        "#                         default=1,\n",
        "#                         type=int,\n",
        "#                         help=\"Total batch size for training.\")\n",
        "#     parser.add_argument(\"--eval_batch_size\",\n",
        "#                         default=1,\n",
        "#                         type=int,\n",
        "#                         help=\"Total batch size for eval.\")\n",
        "#     parser.add_argument(\"--learning_rate\",\n",
        "#                         default=1e-7,\n",
        "#                         type=float,\n",
        "#                         help=\"The initial learning rate for Adam.\")\n",
        "#     parser.add_argument(\"--warmup_proportion\",\n",
        "#                         default=0.1,\n",
        "#                         type=float,\n",
        "#                         help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "#                              \"E.g., 0.1 = 10%% of training.\")\n",
        "#     parser.add_argument(\"--num_train_epochs\",\n",
        "#                         default=3,\n",
        "#                         type=int,\n",
        "#                         help=\"Total number of training epochs to perform.\")\n",
        "#     parser.add_argument('--seed',\n",
        "#                         type=int,\n",
        "#                         default=42,\n",
        "#                         help=\"random seed for initialization\")\n",
        "#     parser.add_argument('--gradient_accumulation_steps',\n",
        "#                         type=int,\n",
        "#                         default=1,\n",
        "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.save_model:\n",
        "#         raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "#     os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "#     LOG_PATH = args.log_path\n",
        "#     MAX_LEN = args.max_seq_length\n",
        "\n",
        "#     config = DotMap()\n",
        "#     config.hidden_dropout_prob = 0.1\n",
        "#     config.layer_norm_eps = 1e-12\n",
        "#     config.initializer_range = 0.02\n",
        "#     config.max_note_position_embedding = 1000\n",
        "#     config.max_chunk_position_embedding = 1000\n",
        "#     config.embed_mode = args.embed_mode\n",
        "#     config.layer_norm_eps = 1e-12\n",
        "#     config.hidden_size = 768\n",
        "\n",
        "#     config.task_name = args.task_name\n",
        "\n",
        "#     write_log((\"New Job Start! \\n\"\n",
        "#                \"Data directory: {}, Directory Code: {}, Save Model: {}\\n\"\n",
        "#                \"Output_dir: {}, Task Name: {}, embed_mode: {}\\n\"\n",
        "#                \"max_seq_length: {},  max_chunk_num: {}\\n\"\n",
        "#                \"train_batch_size: {}, eval_batch_size: {}\\n\"\n",
        "#                \"learning_rate: {}, warmup_proportion: {}\\n\"\n",
        "#                \"num_train_epochs: {}, seed: {}, gradient_accumulation_steps: {}\").format(args.data_dir,\n",
        "#                                                                                          args.data_dir.split('_')[-1],\n",
        "#                                                                                          args.save_model,\n",
        "#                                                                                          args.output_dir,\n",
        "#                                                                                          config.task_name,\n",
        "#                                                                                          config.embed_mode,\n",
        "#                                                                                          args.max_seq_length,\n",
        "#                                                                                          args.max_chunk_num,\n",
        "#                                                                                          args.train_batch_size,\n",
        "#                                                                                          args.eval_batch_size,\n",
        "#                                                                                          args.learning_rate,\n",
        "#                                                                                          args.warmup_proportion,\n",
        "#                                                                                          args.num_train_epochs,\n",
        "#                                                                                          args.seed,\n",
        "#                                                                                          args.gradient_accumulation_steps),\n",
        "#               LOG_PATH)\n",
        "\n",
        "#     content = \"config setting: \\n\"\n",
        "#     for k, v in config.items():\n",
        "#         content += \"{}: {} \\n\".format(k, v)\n",
        "#     write_log(content, LOG_PATH)\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     n_gpu = torch.cuda.device_count()\n",
        "#     write_log(\"Number of GPU is {}\".format(n_gpu), LOG_PATH)\n",
        "#     for i in range(n_gpu):\n",
        "#         write_log((\"Device Name: {},\"\n",
        "#                    \"Device Capability: {}\").format(torch.cuda.get_device_name(i),\n",
        "#                                                    torch.cuda.get_device_capability(i)), LOG_PATH)\n",
        "\n",
        "#     train_file_path = os.path.join(args.data_dir, args.train_data)\n",
        "#     val_file_path = os.path.join(args.data_dir, args.val_data)\n",
        "#     test_file_path = os.path.join(args.data_dir, args.test_data)\n",
        "#     train_df = pd.read_csv(train_file_path)\n",
        "#     val_df = pd.read_csv(val_file_path)\n",
        "#     test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "#     random.seed(args.seed)\n",
        "#     np.random.seed(args.seed)\n",
        "#     torch.manual_seed(args.seed)\n",
        "#     if n_gpu > 0:\n",
        "#         torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#     tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n",
        "\n",
        "#     write_log(\"Tokenize Start!\", LOG_PATH)\n",
        "#     train_labels, train_inputs, train_masks, train_note_ids = Tokenize_with_note_id(train_df, MAX_LEN, tokenizer)\n",
        "#     validation_labels, validation_inputs, validation_masks, validation_note_ids = Tokenize_with_note_id(val_df, MAX_LEN,\n",
        "#                                                                                                         tokenizer)\n",
        "#     test_labels, test_inputs, test_masks, test_note_ids = Tokenize_with_note_id(test_df, MAX_LEN, tokenizer)\n",
        "#     write_log(\"Tokenize Finished!\", LOG_PATH)\n",
        "#     train_inputs = torch.tensor(train_inputs)\n",
        "#     validation_inputs = torch.tensor(validation_inputs)\n",
        "#     test_inputs = torch.tensor(test_inputs)\n",
        "#     train_labels = torch.tensor(train_labels)\n",
        "#     validation_labels = torch.tensor(validation_labels)\n",
        "#     test_labels = torch.tensor(test_labels)\n",
        "#     train_masks = torch.tensor(train_masks)\n",
        "#     validation_masks = torch.tensor(validation_masks)\n",
        "#     test_masks = torch.tensor(test_masks)\n",
        "#     write_log((\"train dataset size is %d,\\n\"\n",
        "#                \"validation dataset size is %d,\\n\"\n",
        "#                \"test dataset size is %d\") % (len(train_inputs), len(validation_inputs), len(test_inputs)), LOG_PATH)\n",
        "\n",
        "#     (train_labels, train_inputs,\n",
        "#      train_masks, train_ids,\n",
        "#      train_note_ids, train_chunk_ids) = concat_by_id_list_with_note_chunk_id(train_df, train_labels,\n",
        "#                                                                              train_inputs, train_masks,\n",
        "#                                                                              train_note_ids, MAX_LEN)\n",
        "#     (validation_labels, validation_inputs,\n",
        "#      validation_masks, validation_ids,\n",
        "#      validation_note_ids, validation_chunk_ids) = concat_by_id_list_with_note_chunk_id(val_df, validation_labels,\n",
        "#                                                                                        validation_inputs,\n",
        "#                                                                                        validation_masks,\n",
        "#                                                                                        validation_note_ids, MAX_LEN)\n",
        "#     (test_labels, test_inputs,\n",
        "#      test_masks, test_ids,\n",
        "#      test_note_ids, test_chunk_ids) = concat_by_id_list_with_note_chunk_id(test_df, test_labels,\n",
        "#                                                                            test_inputs, test_masks,\n",
        "#                                                                            test_note_ids, MAX_LEN)\n",
        "\n",
        "#     model = BertForSequenceClassification.from_pretrained(args.bert_model, num_labels=2)\n",
        "#     model.to(device)\n",
        "#     if n_gpu > 1:\n",
        "#         model = torch.nn.DataParallel(model)\n",
        "#     param_optimizer = list(model.named_parameters())\n",
        "#     no_decay = ['bias', 'gamma', 'beta']\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "#          'weight_decay_rate': 0.01},\n",
        "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "#          'weight_decay_rate': 0.0}\n",
        "#     ]\n",
        "#     num_train_steps = int(\n",
        "#         len(train_df) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n",
        "\n",
        "#     optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "#                          lr=args.learning_rate,\n",
        "#                          warmup=args.warmup_proportion,\n",
        "#                          t_total=num_train_steps)\n",
        "\n",
        "#     m = torch.nn.Softmax(dim=1)\n",
        "\n",
        "#     start = time.time()\n",
        "#     # Store our loss and accuracy for plotting\n",
        "#     train_loss_set = []\n",
        "\n",
        "#     # Number of training epochs (authors recommend between 2 and 4)\n",
        "#     epochs = args.num_train_epochs\n",
        "\n",
        "#     train_batch_generator = mask_batch_generator(args.max_chunk_num, train_inputs, train_labels, train_masks)\n",
        "#     validation_batch_generator = mask_batch_generator(args.max_chunk_num, validation_inputs, validation_labels,\n",
        "#                                                       validation_masks)\n",
        "\n",
        "#     write_log(\"Training start!\", LOG_PATH)\n",
        "#     # trange is a tqdm wrapper around the normal python range\n",
        "#     with torch.autograd.set_detect_anomaly(True):\n",
        "#         for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "#             # Training\n",
        "\n",
        "#             # Set our model to training mode (as opposed to evaluation mode)\n",
        "#             model.train()\n",
        "\n",
        "#             # Tracking variables\n",
        "#             tr_loss = 0\n",
        "#             nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "#             # Train the data for one epoch\n",
        "#             tr_ids_num = len(train_ids)\n",
        "#             tr_batch_loss = []\n",
        "#             for step in range(tr_ids_num):\n",
        "#                 b_input_ids, b_labels, b_input_mask = next(train_batch_generator)\n",
        "#                 b_input_ids = b_input_ids.to(device)\n",
        "#                 b_input_mask = b_input_mask.to(device)\n",
        "#                 b_labels = b_labels.repeat(b_input_ids.shape[0]).to(device)\n",
        "#                 # Forward pass\n",
        "#                 outputs = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask, labels=b_labels.long())\n",
        "#                 loss, logits = outputs[:2]\n",
        "#                 if n_gpu > 1:\n",
        "#                     loss = loss.mean()  # mean() to average on multi-gpu.\n",
        "#                 train_loss_set.append(loss.item())\n",
        "#                 # Backward pass\n",
        "#                 loss.backward()\n",
        "#                 # Update parameters and take a step using the computed gradient\n",
        "#                 if (step + 1) % args.train_batch_size == 0:\n",
        "#                     optimizer.step()\n",
        "#                     optimizer.zero_grad()\n",
        "#                     train_loss_set.append(np.mean(tr_batch_loss))\n",
        "#                     tr_batch_loss = []\n",
        "\n",
        "#                 # Update tracking variables\n",
        "#                 tr_loss += loss.item()\n",
        "#                 nb_tr_examples += b_input_ids.size(0)\n",
        "#                 nb_tr_steps += 1\n",
        "\n",
        "\n",
        "#             write_log(\"Train loss: {}\".format(tr_loss / nb_tr_steps), LOG_PATH)\n",
        "\n",
        "#             # Validation\n",
        "\n",
        "#             # Put model in evaluation mode to evaluate loss on the validation set\n",
        "#             model.eval()\n",
        "\n",
        "#             # Tracking variables\n",
        "#             eval_loss, eval_accuracy = 0, 0\n",
        "#             nb_eval_steps, nb_eval_examples = 0, 0\n",
        "#             # Evaluate data for one epoch\n",
        "#             ev_ids_num = len(validation_ids)\n",
        "#             for step in range(ev_ids_num):\n",
        "#                 with torch.no_grad():\n",
        "#                     b_input_ids, b_labels, b_input_mask = next(validation_batch_generator)\n",
        "#                     b_input_ids = b_input_ids.to(device)\n",
        "#                     b_input_mask = b_input_mask.to(device)\n",
        "#                     b_labels = b_labels.repeat(b_input_ids.shape[0])\n",
        "#                     outputs = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "#                     # Move logits and labels to CPU\n",
        "#                     logits = outputs[-1]\n",
        "#                     logits = m(logits).detach().cpu().numpy()[:, 1]\n",
        "#                     label_ids = b_labels.numpy()\n",
        "\n",
        "#                     tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "#                     eval_accuracy += tmp_eval_accuracy\n",
        "#                     nb_eval_steps += 1\n",
        "\n",
        "#             write_log(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps), LOG_PATH)\n",
        "#             output_checkpoints_path = os.path.join(args.output_dir,\n",
        "#                                                    \"bert_fine_tuned_with_note_checkpoint_%d.pt\" % epoch)\n",
        "#             if args.save_model:\n",
        "#                 if n_gpu > 1:\n",
        "#                     torch.save({\n",
        "#                         'epoch': epoch,\n",
        "#                         'model_state_dict': model.module.state_dict(),\n",
        "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                         'loss': loss,\n",
        "#                     },\n",
        "#                         output_checkpoints_path)\n",
        "\n",
        "#                 else:\n",
        "#                     torch.save({\n",
        "#                         'epoch': epoch,\n",
        "#                         'model_state_dict': model.state_dict(),\n",
        "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                         'loss': loss,\n",
        "#                     },\n",
        "#                         output_checkpoints_path)\n",
        "#     end = time.time()\n",
        "\n",
        "#     write_log(\"total training time is: {}s\".format(end - start), LOG_PATH)\n",
        "\n",
        "#     fig1 = plt.figure(figsize=(15, 8))\n",
        "#     plt.title(\"Training loss\")\n",
        "#     plt.xlabel(\"Chunk Batch\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.plot(train_loss_set)\n",
        "#     if args.save_model:\n",
        "#         output_fig_path = os.path.join(args.output_dir, \"bert_fine_tuned_with_note_training_loss.png\")\n",
        "#         plt.savefig(output_fig_path, dpi=fig1.dpi)\n",
        "#         output_model_state_dict_path = os.path.join(args.output_dir,\n",
        "#                                                     \"bert_fine_tuned_with_note_state_dict.pt\")\n",
        "#         if n_gpu > 1:\n",
        "#             torch.save(model.module.state_dict(), output_model_state_dict_path)\n",
        "#         else:\n",
        "#             torch.save(model.state_dict(), output_model_state_dict_path)\n",
        "#         write_log(\"Model saved!\", LOG_PATH)\n",
        "#     else:\n",
        "#         output_fig_path = os.path.join(args.output_dir,\n",
        "#                                        \"bert_fine_tuned_with_note_training_loss_{}_{}.png\".format(\n",
        "#                                            args.seed,\n",
        "#                                            args.data_dir.split(\n",
        "#                                                '_')[-1]))\n",
        "#         plt.savefig(output_fig_path, dpi=fig1.dpi)\n",
        "#         write_log(\"Model not saved as required\", LOG_PATH)\n",
        "\n",
        "#     # Prediction on test set\n",
        "\n",
        "#     # Put model in evaluation mode\n",
        "#     model.eval()\n",
        "\n",
        "#     # Tracking variables\n",
        "#     predictions, true_labels, test_adm_ids = [], [], []\n",
        "\n",
        "#     # Predict\n",
        "#     te_ids_num = len(test_ids)\n",
        "#     for step in range(te_ids_num):\n",
        "#         b_input_ids = test_inputs[step][-args.max_chunk_num:, :].to(device)\n",
        "#         b_input_mask = test_masks[step][-args.max_chunk_num:, :].to(device)\n",
        "#         b_labels = test_labels[step].repeat(b_input_ids.shape[0])\n",
        "#         # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n",
        "#         with torch.no_grad():\n",
        "#             # Forward pass, calculate logit predictions\n",
        "#             outputs = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "\n",
        "#         # Move logits and labels to CPU\n",
        "#         logits = outputs[-1]\n",
        "#         logits = m(logits).detach().cpu().numpy()[:, 1]\n",
        "#         label_ids = b_labels.numpy()\n",
        "#         adm_ids = test_ids[step].repeat(b_input_ids.shape[0])\n",
        "\n",
        "#         # Store predictions and true labels\n",
        "#         predictions.append(logits)\n",
        "#         true_labels.append(label_ids)\n",
        "#         test_adm_ids.append(adm_ids)\n",
        "\n",
        "#     try:\n",
        "#         flat_logits = [item for sublist in predictions for item in sublist]\n",
        "#     except TypeError:\n",
        "#         flat_logits = [item for sublist in predictions for item in test_func(sublist)]\n",
        "#     flat_predictions = (np.array(flat_logits) >= 0.5).astype(np.int)\n",
        "#     try:\n",
        "#         flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "#     except TypeError:\n",
        "#         flat_true_labels = [item for sublist in true_labels for item in test_func(sublist)]\n",
        "#     try:\n",
        "#         flat_adm_ids = [item for sublist in test_adm_ids for item in sublist]\n",
        "#     except TypeError:\n",
        "#         flat_adm_ids = [item for sublist in test_adm_ids for item in test_func(sublist)]\n",
        "\n",
        "#     output_chunk_df = pd.DataFrame({'logits': flat_logits,\n",
        "#                                     'pred_label': flat_predictions,\n",
        "#                                     'label': flat_true_labels,\n",
        "#                                     'Adm_ID': flat_adm_ids})\n",
        "\n",
        "#     if args.save_model:\n",
        "#         output_chunk_df.to_csv(os.path.join(args.output_dir, 'test_chunk_predictions.csv'), index=False)\n",
        "#     else:\n",
        "#         output_chunk_df.to_csv(os.path.join(args.output_dir,\n",
        "#                                             'test_chunk_predictions_{}_{}.csv'.format(args.seed,\n",
        "#                                                                                       args.data_dir.split('_')[-1])),\n",
        "#                                index=False)\n",
        "\n",
        "#     output_df = get_patient_score(output_chunk_df, args.c)\n",
        "#     if args.save_model:\n",
        "#         output_df.to_csv(os.path.join(args.output_dir, 'test_predictions.csv'), index=False)\n",
        "#     else:\n",
        "#         output_df.to_csv(os.path.join(args.output_dir,\n",
        "#                                       'test_predictions_{}_{}.csv'.format(args.seed,\n",
        "#                                                                           args.data_dir.split('_')[-1])),\n",
        "#                          index=False)\n",
        "#     write_performance(output_df['label'].values, output_df['pred_label'].values,\n",
        "#                       output_df['logits'].values, config, args)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "-qORxq-VmGcS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #!/usr/bin/env python3\n",
        "# # -*- coding: utf-8 -*-\n",
        "# \"\"\"\n",
        "# @author: Dongyu Zhang\n",
        "# \"\"\"\n",
        "\n",
        "# import argparse\n",
        "# import os\n",
        "# import random\n",
        "# import time\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "# from dotmap import DotMap\n",
        "# from pytorch_pretrained_bert.optimization import BertAdam\n",
        "# from pytorch_transformers import BertTokenizer\n",
        "# from tqdm import trange\n",
        "# from modeling_patient import TLSTMLayer\n",
        "# from modeling_readmission import BertModel\n",
        "# from other_func import convert_note_ids, flat_accuracy, write_performance, reorder_by_time\n",
        "# from other_func import write_log, Tokenize_with_note_id_hour, concat_by_id_list_with_note_chunk_id_time\n",
        "# from utils import time_batch_generator\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     ## Required parameters\n",
        "#     parser.add_argument(\"--data_dir\",\n",
        "#                         default='./Ecoli',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--train_data\",\n",
        "#                         default='train.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input training data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--val_data\",\n",
        "#                         default='val.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input validation data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--test_data\",\n",
        "#                         default='test.csv',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The input test data file name.\"\n",
        "#                              \" Should be the .tsv file (or other data file) for the task.\")\n",
        "\n",
        "#     parser.add_argument(\"--log_path\",\n",
        "#                         default='./log.txt',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The log file path.\")\n",
        "\n",
        "#     parser.add_argument(\"--output_dir\",\n",
        "#                         default='./exp_FTL-Trans2',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The output directory where the model checkpoints will be written.\")\n",
        "\n",
        "#     parser.add_argument(\"--save_model\",\n",
        "#                         default=True,\n",
        "#                         action='store_true',\n",
        "#                         help=\"Whether to save the model.\")\n",
        "\n",
        "#     parser.add_argument(\"--bert_model\",\n",
        "#                         default=\"bert-base-uncased\",\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"Bert pre-trained model selected in the list: bert-base-uncased, \"\n",
        "#                              \"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\")\n",
        "\n",
        "#     parser.add_argument(\"--embed_mode\",\n",
        "#                         default='no',\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The embedding type selected in the list: all, note, chunk, no.\")\n",
        "\n",
        "#     parser.add_argument(\"--task_name\",\n",
        "#                         default=\"TLSTM_with_ClBERT_ecoli\",\n",
        "#                         type=str,\n",
        "#                         required=False,\n",
        "#                         help=\"The name of the task.\")\n",
        "\n",
        "#     ## Other parameters\n",
        "#     parser.add_argument(\"--max_seq_length\",\n",
        "#                         default=64,\n",
        "#                         type=int,\n",
        "#                         help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
        "#                              \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
        "#                              \"than this will be padded.\")\n",
        "#     parser.add_argument(\"--max_chunk_num\",\n",
        "#                         default=32,\n",
        "#                         type=int,\n",
        "#                         help=\"The maximum total input chunk numbers after WordPiece tokenization.\")\n",
        "#     parser.add_argument(\"--train_batch_size\",\n",
        "#                         default=1,\n",
        "#                         type=int,\n",
        "#                         help=\"Total batch size for training.\")\n",
        "#     parser.add_argument(\"--eval_batch_size\",\n",
        "#                         default=1,\n",
        "#                         type=int,\n",
        "#                         help=\"Total batch size for eval.\")\n",
        "#     parser.add_argument(\"--learning_rate\",\n",
        "#                         default=1e-7,\n",
        "#                         type=float,\n",
        "#                         help=\"The initial learning rate for Adam.\")\n",
        "#     parser.add_argument(\"--warmup_proportion\",\n",
        "#                         default=0.1,\n",
        "#                         type=float,\n",
        "#                         help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
        "#                              \"E.g., 0.1 = 10%% of training.\")\n",
        "#     parser.add_argument(\"--num_train_epochs\",\n",
        "#                         default=3,\n",
        "#                         type=int,\n",
        "#                         help=\"Total number of training epochs to perform.\")\n",
        "#     parser.add_argument('--seed',\n",
        "#                         type=int,\n",
        "#                         default=42,\n",
        "#                         help=\"random seed for initialization\")\n",
        "#     parser.add_argument('--gradient_accumulation_steps',\n",
        "#                         type=int,\n",
        "#                         default=1,\n",
        "#                         help=\"Number of updates steps to accumualte before performing a backward/update pass.\")\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "#     if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.save_model:\n",
        "#         raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
        "#     os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "#     LOG_PATH = args.log_path\n",
        "#     MAX_LEN = args.max_seq_length\n",
        "\n",
        "#     config = DotMap()\n",
        "#     config.hidden_dropout_prob = 0.1\n",
        "#     config.layer_norm_eps = 1e-12\n",
        "#     config.initializer_range = 0.02\n",
        "#     config.max_note_position_embedding = 1000\n",
        "#     config.max_chunk_position_embedding = 1000\n",
        "#     config.embed_mode = args.embed_mode\n",
        "#     config.layer_norm_eps = 1e-12\n",
        "#     config.hidden_size = 768\n",
        "#     config.lstm_layers = 1\n",
        "\n",
        "#     config.task_name = args.task_name\n",
        "\n",
        "#     write_log((\"New Job Start! \\n\"\n",
        "#                \"Data directory: {}, Directory Code: {}, Save Model: {}\\n\"\n",
        "#                \"Output_dir: {}, Task Name: {}, embed_mode: {}\\n\"\n",
        "#                \"max_seq_length: {},  max_chunk_num: {}\\n\"\n",
        "#                \"train_batch_size: {}, eval_batch_size: {}\\n\"\n",
        "#                \"learning_rate: {}, warmup_proportion: {}\\n\"\n",
        "#                \"num_train_epochs: {}, seed: {}, gradient_accumulation_steps: {}\\n\"\n",
        "#                \"TLSTM Model's lstm_layers: {}\").format(args.data_dir,\n",
        "#                                                        args.data_dir.split('_')[-1],\n",
        "#                                                        args.save_model,\n",
        "#                                                        args.output_dir,\n",
        "#                                                        config.task_name,\n",
        "#                                                        config.embed_mode,\n",
        "#                                                        args.max_seq_length,\n",
        "#                                                        args.max_chunk_num,\n",
        "#                                                        args.train_batch_size,\n",
        "#                                                        args.eval_batch_size,\n",
        "#                                                        args.learning_rate,\n",
        "#                                                        args.warmup_proportion,\n",
        "#                                                        args.num_train_epochs,\n",
        "#                                                        args.seed,\n",
        "#                                                        args.gradient_accumulation_steps,\n",
        "#                                                        config.lstm_layers),\n",
        "#               LOG_PATH)\n",
        "\n",
        "#     content = \"config setting: \\n\"\n",
        "#     for k, v in config.items():\n",
        "#         content += \"{}: {} \\n\".format(k, v)\n",
        "#     write_log(content, LOG_PATH)\n",
        "\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     n_gpu = torch.cuda.device_count()\n",
        "#     write_log(\"Number of GPU is {}\".format(n_gpu), LOG_PATH)\n",
        "#     for i in range(n_gpu):\n",
        "#         write_log((\"Device Name: {},\"\n",
        "#                    \"Device Capability: {}\").format(torch.cuda.get_device_name(i),\n",
        "#                                                    torch.cuda.get_device_capability(i)), LOG_PATH)\n",
        "\n",
        "#     train_file_path = os.path.join(args.data_dir, args.train_data)\n",
        "#     val_file_path = os.path.join(args.data_dir, args.val_data)\n",
        "#     test_file_path = os.path.join(args.data_dir, args.test_data)\n",
        "#     train_df = pd.read_csv(train_file_path)\n",
        "#     val_df = pd.read_csv(val_file_path)\n",
        "#     test_df = pd.read_csv(test_file_path)\n",
        "\n",
        "#     random.seed(args.seed)\n",
        "#     np.random.seed(args.seed)\n",
        "#     torch.manual_seed(args.seed)\n",
        "#     if n_gpu > 0:\n",
        "#         torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#     tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n",
        "\n",
        "#     write_log(\"Tokenize Start!\", LOG_PATH)\n",
        "#     train_df = reorder_by_time(train_df)\n",
        "#     val_df = reorder_by_time(val_df)\n",
        "#     test_df = reorder_by_time(test_df)\n",
        "#     train_labels, train_inputs, train_masks, train_note_ids, train_times = Tokenize_with_note_id_hour(train_df, MAX_LEN,\n",
        "#                                                                                                       tokenizer)\n",
        "#     validation_labels, validation_inputs, validation_masks, validation_note_ids, validation_times = Tokenize_with_note_id_hour(\n",
        "#         val_df, MAX_LEN, tokenizer)\n",
        "#     test_labels, test_inputs, test_masks, test_note_ids, test_times = Tokenize_with_note_id_hour(test_df, MAX_LEN,\n",
        "#                                                                                                  tokenizer)\n",
        "#     write_log(\"Tokenize Finished!\", LOG_PATH)\n",
        "#     train_inputs = torch.tensor(train_inputs)\n",
        "#     validation_inputs = torch.tensor(validation_inputs)\n",
        "#     test_inputs = torch.tensor(test_inputs)\n",
        "#     train_labels = torch.tensor(train_labels)\n",
        "#     validation_labels = torch.tensor(validation_labels)\n",
        "#     test_labels = torch.tensor(test_labels)\n",
        "#     train_masks = torch.tensor(train_masks)\n",
        "#     validation_masks = torch.tensor(validation_masks)\n",
        "#     test_masks = torch.tensor(test_masks)\n",
        "#     train_times = torch.tensor(train_times)\n",
        "#     validation_times = torch.tensor(validation_times)\n",
        "#     test_times = torch.tensor(test_times)\n",
        "#     write_log((\"train dataset size is %d,\\n\"\n",
        "#                \"validation dataset size is %d,\\n\"\n",
        "#                \"test dataset size is %d\") % (len(train_inputs), len(validation_inputs), len(test_inputs)), LOG_PATH)\n",
        "\n",
        "#     (train_labels, train_inputs,\n",
        "#      train_masks, train_ids,\n",
        "#      train_note_ids, train_chunk_ids, train_times) = concat_by_id_list_with_note_chunk_id_time(train_df, train_labels,\n",
        "#                                                                                                train_inputs,\n",
        "#                                                                                                train_masks,\n",
        "#                                                                                                train_note_ids,\n",
        "#                                                                                                train_times, MAX_LEN)\n",
        "#     (validation_labels, validation_inputs,\n",
        "#      validation_masks, validation_ids,\n",
        "#      validation_note_ids, validation_chunk_ids,\n",
        "#      validation_times) = concat_by_id_list_with_note_chunk_id_time(val_df, validation_labels,\n",
        "#                                                                    validation_inputs, validation_masks,\n",
        "#                                                                    validation_note_ids, validation_times,\n",
        "#                                                                    MAX_LEN)\n",
        "#     (test_labels, test_inputs,\n",
        "#      test_masks, test_ids,\n",
        "#      test_note_ids, test_chunk_ids, test_times) = concat_by_id_list_with_note_chunk_id_time(test_df, test_labels,\n",
        "#                                                                                             test_inputs, test_masks,\n",
        "#                                                                                             test_note_ids, test_times,\n",
        "#                                                                                             MAX_LEN)\n",
        "\n",
        "#     model = BertModel.from_pretrained(args.bert_model).to(device)\n",
        "#     model.to(device)\n",
        "#     lstm_layer = TLSTMLayer(config=config, num_labels=1)\n",
        "#     lstm_layer.to(device)\n",
        "\n",
        "#     if n_gpu > 1:\n",
        "#         model = torch.nn.DataParallel(model)\n",
        "#         lstm_layer = torch.nn.DataParallel(lstm_layer)\n",
        "#     param_optimizer = list(model.named_parameters()) + list(lstm_layer.named_parameters())\n",
        "#     no_decay = ['bias', 'gamma', 'beta']\n",
        "#     optimizer_grouped_parameters = [\n",
        "#         {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "#          'weight_decay_rate': 0.01},\n",
        "#         {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "#          'weight_decay_rate': 0.0}\n",
        "#     ]\n",
        "\n",
        "#     num_train_steps = int(\n",
        "#         len(train_labels) / args.gradient_accumulation_steps * args.num_train_epochs)\n",
        "\n",
        "#     optimizer = BertAdam(optimizer_grouped_parameters,\n",
        "#                          lr=args.learning_rate,\n",
        "#                          warmup=args.warmup_proportion,\n",
        "#                          t_total=num_train_steps)\n",
        "#     start = time.time()\n",
        "#     # Store our loss and accuracy for plotting\n",
        "#     train_loss_set = []\n",
        "\n",
        "#     # Number of training epochs (authors recommend between 2 and 4)\n",
        "#     epochs = args.num_train_epochs\n",
        "\n",
        "#     train_batch_generator = time_batch_generator(args.max_chunk_num, train_inputs, train_labels, train_masks,\n",
        "#                                                  train_note_ids, train_chunk_ids, train_times)\n",
        "#     validation_batch_generator = time_batch_generator(args.max_chunk_num, validation_inputs, validation_labels,\n",
        "#                                                       validation_masks, validation_note_ids, validation_chunk_ids,\n",
        "#                                                       validation_times)\n",
        "\n",
        "#     write_log(\"Training start!\", LOG_PATH)\n",
        "#     # trange is a tqdm wrapper around the normal python range\n",
        "#     with torch.autograd.set_detect_anomaly(True):\n",
        "#         for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "#             # Training\n",
        "\n",
        "#             # Set our model to training mode (as opposed to evaluation mode)\n",
        "#             model.train()\n",
        "#             lstm_layer.train()\n",
        "\n",
        "#             # Tracking variables\n",
        "#             tr_loss = 0\n",
        "#             nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "#             # Train the data for one epoch\n",
        "#             tr_ids_num = len(train_ids)\n",
        "#             tr_batch_loss = []\n",
        "#             for step in range(tr_ids_num):\n",
        "#                 b_input_ids, b_labels, b_input_mask, b_note_ids, b_chunk_ids, b_times = next(train_batch_generator)\n",
        "#                 b_input_ids = b_input_ids.to(device)\n",
        "#                 b_input_mask = b_input_mask.to(device)\n",
        "#                 b_new_note_ids = convert_note_ids(b_note_ids).to(device)\n",
        "#                 b_chunk_ids = b_chunk_ids.unsqueeze(0).to(device)\n",
        "#                 b_labels = b_labels.to(device)\n",
        "#                 b_labels.resize_((1))\n",
        "#                 _, whole_output = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "#                 whole_input = whole_output.unsqueeze(0)\n",
        "#                 b_new_note_ids = b_new_note_ids.unsqueeze(0)\n",
        "#                 b_times = b_times.unsqueeze(0).to(device)\n",
        "#                 loss, pred = lstm_layer(whole_input, b_times, b_new_note_ids, b_chunk_ids, b_labels)\n",
        "\n",
        "#                 if n_gpu > 1:\n",
        "#                     loss = loss.mean()  # mean() to average on multi-gpu.\n",
        "#                 tr_batch_loss.append(loss.item())\n",
        "\n",
        "#                 # Backward pass\n",
        "#                 loss.backward()\n",
        "#                 # Update parameters and take a step using the computed gradient\n",
        "#                 if (step + 1) % args.train_batch_size == 0:\n",
        "#                     optimizer.step()\n",
        "#                     optimizer.zero_grad()\n",
        "#                     train_loss_set.append(np.mean(tr_batch_loss))\n",
        "#                     tr_batch_loss = []\n",
        "#                 # Update tracking variables\n",
        "#                 tr_loss += loss.item()\n",
        "#                 nb_tr_examples += b_input_ids.size(0)\n",
        "#                 nb_tr_steps += 1\n",
        "\n",
        "#             write_log(\"Train loss: {}\".format(tr_loss / nb_tr_steps), LOG_PATH)\n",
        "#             # Validation\n",
        "\n",
        "#             # Put model in evaluation mode to evaluate loss on the validation set\n",
        "#             model.eval()\n",
        "#             lstm_layer.eval()\n",
        "\n",
        "#             # Tracking variables\n",
        "#             eval_loss, eval_accuracy = 0, 0\n",
        "#             nb_eval_steps, nb_eval_examples = 0, 0\n",
        "#             # Evaluate data for one epoch\n",
        "#             ev_ids_num = len(validation_ids)\n",
        "#             for step in range(ev_ids_num):\n",
        "#                 with torch.no_grad():\n",
        "#                     b_input_ids, b_labels, b_input_mask, b_note_ids, b_chunk_ids, b_times = next(\n",
        "#                         validation_batch_generator)\n",
        "#                     b_input_ids = b_input_ids.to(device)\n",
        "#                     b_input_mask = b_input_mask.to(device)\n",
        "#                     b_new_note_ids = convert_note_ids(b_note_ids).to(device)\n",
        "#                     b_chunk_ids = b_chunk_ids.unsqueeze(0).to(device)\n",
        "#                     b_labels.resize_((1))\n",
        "#                     _, whole_output = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "#                     whole_input = whole_output.unsqueeze(0)\n",
        "#                     b_new_note_ids = b_new_note_ids.unsqueeze(0)\n",
        "#                     b_times = b_times.unsqueeze(0).to(device)\n",
        "#                     pred = lstm_layer(whole_input, b_times, b_new_note_ids, b_chunk_ids).detach().cpu().numpy()\n",
        "#                 label_ids = b_labels.numpy()\n",
        "#                 tmp_eval_accuracy = flat_accuracy(pred, label_ids)\n",
        "#                 eval_accuracy += tmp_eval_accuracy\n",
        "#                 nb_eval_steps += 1\n",
        "\n",
        "#             write_log(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps), LOG_PATH)\n",
        "#             output_checkpoints_path = os.path.join(args.output_dir,\n",
        "#                                                    \"bert_fine_tuned_with_note_checkpoint_%d.pt\" % epoch)\n",
        "\n",
        "#             if args.save_model:\n",
        "#                 if n_gpu > 1:\n",
        "#                     torch.save({\n",
        "#                         'epoch': epoch,\n",
        "#                         'model_state_dict': model.module.state_dict(),\n",
        "#                         'lstm_layer_state_dict': lstm_layer.module.state_dict(),\n",
        "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                         'loss': loss,\n",
        "#                     },\n",
        "#                         output_checkpoints_path)\n",
        "#                 else:\n",
        "#                     torch.save({\n",
        "#                         'epoch': epoch,\n",
        "#                         'model_state_dict': model.state_dict(),\n",
        "#                         'lstm_layer_state_dict': lstm_layer.state_dict(),\n",
        "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                         'loss': loss,\n",
        "#                     },\n",
        "#                         output_checkpoints_path)\n",
        "#     end = time.time()\n",
        "#     write_log(\"total training time is: {}s\".format(end - start), LOG_PATH)\n",
        "\n",
        "#     fig1 = plt.figure(figsize=(15, 8))\n",
        "#     plt.title(\"Training loss\")\n",
        "#     plt.xlabel(\"Patient Batch\")\n",
        "#     plt.ylabel(\"Loss\")\n",
        "#     plt.plot(train_loss_set)\n",
        "#     if args.save_model:\n",
        "#         output_fig_path = os.path.join(args.output_dir, \"bert_fine_tuned_with_note_training_loss.png\")\n",
        "#         plt.savefig(output_fig_path, dpi=fig1.dpi)\n",
        "#         output_model_state_dict_path = os.path.join(args.output_dir, \"bert_fine_tuned_with_note_state_dict.pt\")\n",
        "#         if n_gpu > 1:\n",
        "#             torch.save({\n",
        "#                 'model_state_dict': model.module.state_dict(),\n",
        "#                 'lstm_layer_state_dict': lstm_layer.module.state_dict(),\n",
        "#             },\n",
        "#                 output_model_state_dict_path)\n",
        "#         else:\n",
        "#             torch.save({\n",
        "#                 'model_state_dict': model.state_dict(),\n",
        "#                 'lstm_layer_state_dict': lstm_layer.state_dict(),\n",
        "#             },\n",
        "#                 output_model_state_dict_path)\n",
        "#         write_log(\"Model saved!\", LOG_PATH)\n",
        "#     else:\n",
        "#         output_fig_path = os.path.join(args.output_dir,\n",
        "#                                        \"bert_fine_tuned_with_note_training_loss_{}_{}.png\".format(args.seed,\n",
        "#                                                                                                   args.data_dir.split(\n",
        "#                                                                                                       '_')[-1]))\n",
        "#         plt.savefig(output_fig_path, dpi=fig1.dpi)\n",
        "#         write_log(\"Model not saved as required\", LOG_PATH)\n",
        "\n",
        "#     # Prediction on test set\n",
        "\n",
        "#     # Put model in evaluation mode\n",
        "#     model.eval()\n",
        "#     lstm_layer.eval()\n",
        "\n",
        "#     # Tracking variables\n",
        "#     predictions, true_labels = [], []\n",
        "\n",
        "#     # Predict\n",
        "#     te_ids_num = len(test_ids)\n",
        "#     for step in range(te_ids_num):\n",
        "#         b_input_ids = test_inputs[step][-args.max_chunk_num:, :].to(device)\n",
        "#         b_input_mask = test_masks[step][-args.max_chunk_num:, :].to(device)\n",
        "#         b_note_ids = test_note_ids[step][-args.max_chunk_num:]\n",
        "#         b_new_note_ids = convert_note_ids(b_note_ids).to(device)\n",
        "#         b_chunk_ids = test_chunk_ids[step][-args.max_chunk_num:].unsqueeze(0).to(device)\n",
        "#         b_labels = test_labels[step]\n",
        "#         b_labels.resize_((1))\n",
        "#         with torch.no_grad():\n",
        "#             _, whole_output = model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "#             whole_input = whole_output.unsqueeze(0)\n",
        "#             b_new_note_ids = b_new_note_ids.unsqueeze(0)\n",
        "#             b_times = test_times[step][-args.max_chunk_num:].unsqueeze(0).to(device)\n",
        "#             pred = lstm_layer(whole_input, b_times, b_new_note_ids, b_chunk_ids).detach().cpu().numpy()\n",
        "#         label_ids = b_labels.numpy()[0]\n",
        "#         predictions.append(pred)\n",
        "#         true_labels.append(label_ids)\n",
        "\n",
        "#     # Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "#     flat_logits = [item for sublist in predictions for item in sublist]\n",
        "#     flat_predictions = np.asarray([1 if i else 0 for i in (np.array(flat_logits) >= 0.5)])\n",
        "#     flat_true_labels = np.asarray(true_labels)\n",
        "\n",
        "#     output_df = pd.DataFrame({'pred_prob': flat_logits,\n",
        "#                               'pred_label': flat_predictions,\n",
        "#                               'label': flat_true_labels,\n",
        "#                               'Adm_ID': test_ids})\n",
        "\n",
        "#     if args.save_model:\n",
        "#         output_df.to_csv(os.path.join(args.output_dir, 'test_predictions.csv'), index=False)\n",
        "#     else:\n",
        "#         output_df.to_csv(os.path.join(args.output_dir,\n",
        "#                                       'test_predictions_{}_{}.csv'.format(args.seed,\n",
        "#                                                                           args.data_dir.split('_')[-1])), index=False)\n",
        "\n",
        "#     write_performance(flat_true_labels, flat_predictions, flat_logits, config, args)\n",
        "\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "mRNkL4jRmJG4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "\n",
        "clbert_tlstm_model_path = '/content/drive/My Drive/Colab Notebooks/model/clbert_tlstm.pt'\n",
        "\n",
        "config = DotMap()\n",
        "config.hidden_dropout_prob = 0.1\n",
        "config.layer_norm_eps = 1e-12\n",
        "config.initializer_range = 0.02\n",
        "config.max_note_position_embedding = 1000\n",
        "config.max_chunk_position_embedding = 1000\n",
        "config.embed_mode = \"all\"\n",
        "config.layer_norm_eps = 1e-12\n",
        "config.hidden_size = 768\n",
        "config.lstm_layers = 1\n",
        "config.task_name = \"test clbert_tlstm\"\n",
        "clbert_tlstm_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "clbert_tlstm_model.load_state_dict(torch.load(clbert_tlstm_model_path, map_location=torch.device('cpu'))['model_state_dict'])\n",
        "clbert_tlstm_model.to(device)\n",
        "lstm_layer = TLSTMLayer(config=config, num_labels=1)\n",
        "lstm_layer.load_state_dict(torch.load(clbert_tlstm_model_path, map_location=torch.device('cpu'))['lstm_layer_state_dict'])\n",
        "lstm_layer.to(device)\n",
        "clbert_tlstm_model.eval()\n",
        "lstm_layer.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCD_su75Kr6a",
        "outputId": "2a7f69ab-8ea8-4e69-a5b2-849a3854df8d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:16<00:00, 25476550.50B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TLSTMLayer(\n",
              "  (tlstm): TLSTM()\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (embeddings): PatientLevelEmbedding(\n",
              "    (note_embedding): Embedding(1000, 768)\n",
              "    (chunk_embedding): Embedding(1000, 768)\n",
              "    (combine_embed_rep): Linear(in_features=2304, out_features=768, bias=True)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIBERT-am (no embedding)\n",
        "\n",
        "Test Patient Level Accuracy: 0.51\n",
        "\n",
        "Test Patient Level F1 Score: 0.6423357664233577\n",
        "\n",
        "Test Patient Level Precision: 0.5057471264367817\n",
        "\n",
        "Test Patient Level Recall: 0.88\n",
        "\n",
        "Test Patient Level AUC: 0.46\n",
        "\n",
        "Test Patient Level Matthew's correlation coefficient: 0.02973505167250263\n",
        "\n",
        "Test Patient Level AUPR: 0.47483659750626145\n"
      ],
      "metadata": {
        "id": "wuHum4yTbOOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ClBERT-am (all embedding)\n",
        "\n",
        "Test Patient Level Accuracy: 0.48\n",
        "\n",
        "Test Patient Level F1 Score: 0.39534883720930236\n",
        "\n",
        "Test Patient Level Precision: 0.4722222222222222\n",
        "\n",
        "Test Patient Level Recall: 0.34\n",
        "\n",
        "Test Patient Level AUC: 0.5036\n",
        "\n",
        "Test Patient Level Matthew's correlation coefficient: -0.041666666666666664\n",
        "\n",
        "Test Patient Level AUPR: 0.49663031225940263\n"
      ],
      "metadata": {
        "id": "fe6TuytKbQYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TL-Trans (all embedding)\n",
        "Test Patient Level Accuracy: 0.5\n",
        "\n",
        "Test Patient Level F1 Score: 0.34210526315789475\n",
        "\n",
        "Test Patient Level Precision: 0.5\n",
        "\n",
        "Test Patient Level Recall: 0.26\n",
        "\n",
        "Test Patient Level AUC: 0.5107999999999999\n",
        "\n",
        "Test Patient Level Matthew's correlation coefficient: 0.0\n",
        "\n",
        "Test Patient Level AUPR: 0.4778072021814784\n",
        "\n"
      ],
      "metadata": {
        "id": "wM8HnA1pbTTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotmap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw3Tf9KXnes_",
        "outputId": "c5ce21db-60ee-4151-99ad-8df5c7bcb0e4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dotmap in /usr/local/lib/python3.10/dist-packages (1.3.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotmap import DotMap\n",
        "import pandas as pd\n",
        "\n",
        "config = DotMap()\n",
        "config.hidden_dropout_prob = 0.1\n",
        "config.layer_norm_eps = 1e-12\n",
        "config.initializer_range = 0.02\n",
        "config.max_note_position_embedding = 1000\n",
        "config.max_chunk_position_embedding = 1000\n",
        "config.embed_mode = \"all\"\n",
        "config.layer_norm_eps = 1e-12\n",
        "config.hidden_size = 768\n",
        "config.lstm_layers = 1\n",
        "config.task_name = \"test clbert_tlstm\"\n",
        "\n",
        "test_file_path = '/content/drive/My Drive/Colab Notebooks/Ecoli/test.csv'\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "test_df = reorder_by_time(test_df)\n",
        "test_labels, test_inputs, test_masks, test_note_ids, test_times = Tokenize_with_note_id_hour(test_df, MAX_LEN,\n",
        "                                                                                                 tokenizer)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "test_inputs = torch.tensor(test_inputs)\n",
        "test_masks = torch.tensor(test_masks)\n",
        "test_times = torch.tensor(test_times)\n",
        "(test_labels, test_inputs,\n",
        "     test_masks, test_ids,\n",
        "     test_note_ids, test_chunk_ids, test_times) = concat_by_id_list_with_note_chunk_id_time(test_df, test_labels,\n",
        "                                                                                            test_inputs, test_masks,\n",
        "                                                                                            test_note_ids, test_times,\n",
        "                                                                                            MAX_LEN)\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "te_ids_num = len(test_ids)\n",
        "for step in range(te_ids_num):\n",
        "        b_input_ids = test_inputs[step][-max_chunk_num:, :].to(device)\n",
        "        b_input_mask = test_masks[step][-max_chunk_num:, :].to(device)\n",
        "        b_note_ids = test_note_ids[step][-max_chunk_num:]\n",
        "        b_new_note_ids = convert_note_ids(b_note_ids).to(device)\n",
        "        b_chunk_ids = test_chunk_ids[step][-max_chunk_num:].unsqueeze(0).to(device)\n",
        "        b_labels = test_labels[step]\n",
        "        b_labels.resize_((1))\n",
        "        with torch.no_grad():\n",
        "            _, whole_output = clbert_tlstm_model(b_input_ids.long(), token_type_ids=None, attention_mask=b_input_mask.long())\n",
        "            whole_input = whole_output.unsqueeze(0)\n",
        "            b_new_note_ids = b_new_note_ids.unsqueeze(0)\n",
        "            b_times = test_times[step][-max_chunk_num:].unsqueeze(0).to(device)\n",
        "            pred = lstm_layer(whole_input, b_times, b_new_note_ids, b_chunk_ids).detach().cpu().numpy()\n",
        "        label_ids = b_labels.numpy()[0]\n",
        "        predictions.append(pred)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n",
        "flat_logits = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.asarray([1 if i else 0 for i in (np.array(flat_logits) >= 0.5)])\n",
        "flat_true_labels = np.asarray(true_labels)\n",
        "\n",
        "output_df = pd.DataFrame({'pred_prob': flat_logits,\n",
        "                              'pred_label': flat_predictions,\n",
        "                              'label': flat_true_labels,\n",
        "                              'Adm_ID': test_ids})\n",
        "\n",
        "write_performance(flat_true_labels, flat_predictions, flat_logits, config)"
      ],
      "metadata": {
        "id": "wMHYWW11Kebw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c8ed624-79e5-4112-81fd-715d6d2d5e1d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Patient Level Accuracy: 0.5\n",
            "Test Patient Level F1 Score: 0.34210526315789475\n",
            "Test Patient Level Precision: 0.5\n",
            "Test Patient Level Recall: 0.26\n",
            "Test Patient Level AUC: 0.5107999999999999 \n",
            "Test Patient Level Matthew's correlation coefficient: 0.0\n",
            "Test Patient Level AUPR: 0.4778072021814784 \n",
            "All Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Comparison with Original Paper:\n",
        "\n",
        "   * The results for the TL-Trans model generally showed improvements over the ClBERT models, indicating the effectiveness of time-aware components in handling clinical data. The results for ClBERT-am suggest that simply adding all embeddings without a proper temporal model slightly improves performance. My two hypothesis align with the original paper.\n",
        "\n",
        "* Experiments Beyond the Original Paper:\n",
        "\n",
        "    * Parameter Sensitivity Analysis: I conducted additional experiments to test the sensitivity of the ClBERT and TL-Trans models to different embedding modes. The results indicate significant variability in model performance with changes in embedding settings, suggesting that the model's ability to generalize from training data might be limited.\n",
        "\n",
        "* Ablation Study\n",
        "    * Removing the custom embedding modes ('no' and 'all') and defaulting to standard pre-trained embeddings resulted in marginally improved stability in predictions but did not significantly enhance overall model performance."
      ],
      "metadata": {
        "id": "f6eZM_hBXeJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TL-Trans model has a 72.00 ± 3.10 AUROC, 64.42 ± 3.85 accuracy, 69.38 ± 2.23 AUPR, which performs better than mine.\n"
      ],
      "metadata": {
        "id": "R-X2dGehoCGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "* Reproducibility: The original paper's results were not reproducible in this attempt. The model failed to train effectively, often predicting uniform values across different inputs. And when it did train, the accuracy was lower than what's in the paper.\n",
        "* Explanation of Negative Results:\n",
        "    * There might be discrepancies in how the clinical notes were preprocessed and transformed into the format necessary for training the model.\n",
        "    * Model Complexity and Overfitting: The hierarchical and time-aware components of the model may introduce complexity that makes the model prone to overfitting, especially on smaller or less diversified training subsets.\n",
        "* Challenges:\n",
        "  * What was easy: Setting up the environment and the basic model architecture.\n",
        "  * What was difficult: The most significant difficulty encountered was in the model's training phase. The model did not learn effectively and showed no improvement over epochs, continually predicting similar outputs for varied inputs. Variations in learning rate, batch size, and other hyperparameters did not resolve the training issues, indicating potential deeper issues with the model architecture or data processing techniques used.\n",
        "* Recommendations: Future work should focus on better parameter tuning and a thorough investigation into the data preprocessing steps.\n",
        "* Next Steps:\n",
        "    * Conduct a thorough analysis of the preprocessing steps to ensure that the data fed into the model aligns with the requirements of the model architecture.\n",
        "    * Use a more systematic approach to hyperparameter tuning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "[1] Zhang, D., Thadajarassiri, J., Sen, C., & Rundensteiner, E. (2020). Time-Aware Transformer-based Network for Clinical Notes Series Prediction. Proceedings of Machine Learning Research, 126, 1-22.\n",
        "\n",
        "[2] Huang, K., Altosaar, J., & Ranganath, R. (2019). ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. ArXiv, abs/1904.05342.\n",
        "\n",
        "[3] Johnson, A. E. W., Pollard, T. J., Shen, L., Li-wei, H. L., Feng, M., Ghassemi, M., ... & Mark, R. G. (2016). MIMIC-III, a freely accessible critical care database. Scientific data, 3(1), 1-9.\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Public GitHub Repo\n",
        "\n",
        "https://github.com/hongbaoshila/DL4H_Team_125"
      ],
      "metadata": {
        "id": "6JfZFXINYxKT"
      }
    }
  ]
}